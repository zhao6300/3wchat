# -*- coding: utf-8 -*-
# -----------------------------------------------------------------------------
# æ–‡ä»¶å: 3w_chat.py
#
# ä½œè€…: ä»Šæ—¥æ¬äº†ä»€ä¹ˆç –
#
# æè¿°: è¿™æ˜¯ä¸€ä¸ªæ•™å­¦ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•æž„å»ºä¸€ä¸ªå¤šè½®å¯¹è¯ç³»ç»Ÿï¼Œé›†æˆç”¨æˆ·ç”»åƒã€æƒ…å¢ƒåˆ†æžã€è¡ŒåŠ¨ç­–ç•¥è§„åˆ’å’Œå¤šæ¨¡æ€å¤„ç†ç­‰åŠŸèƒ½ã€‚
#
# -----------------------------------------------------------------------------


import os
import json
import logging
import textwrap
import re
from enum import Enum
import requests
from typing import List, Dict, Any, Tuple, Iterator, Optional, TypedDict, Annotated
from abc import ABC, abstractmethod
import gradio as gr
from openai import OpenAI, OpenAIError
from pydantic import BaseModel, Field, ValidationError
import base64
import mimetypes
import uuid
from langgraph.graph import StateGraph, END

from langgraph.prebuilt import ToolNode
from langchain_core.tools import tool, BaseTool
from langchain_core.utils.function_calling import convert_to_openai_function
from langchain_core.messages import ToolMessage, AIMessage, HumanMessage, BaseMessage, FunctionMessage, SystemMessage, ChatMessage
from copy import deepcopy
from langchain_google_genai import GoogleGenerativeAI
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())
# ==============================================================================
# é…ç½® (Configuration)
# ==============================================================================
API_KEY = os.getenv("API_KEY", "YOUR_API_KEY")
BASE_URL = os.getenv(
    "BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")

ANALYSIS_MODEL = os.getenv("ANALYSIS_MODEL", "qwen-plus-latest")
STRATEGY_MODEL = os.getenv("STRATEGY_MODEL", "qwen-plus-latest")
RESPONSE_MODEL = os.getenv("RESPONSE_MODEL", "qwen-plus-latest")
SUMMARY_MODEL = os.getenv("SUMMARY_MODEL", "qwen-plus-latest")
RETRIEVAL_MODEL = os.getenv("RETRIEVAL_MODEL", "qwen-plus-latest")
PROFILE_MODEL = os.getenv("PROFILE_MODEL", "qwen-plus-latest")
VISION_MODEL = os.getenv("VISION_MODEL", "qwen-vl-plus")

HISTORY_COMPRESSION_THRESHOLD = int(
    os.getenv("HISTORY_COMPRESSION_THRESHOLD", 20))
MESSAGES_TO_KEEP_UNCOMPRESSED = int(
    os.getenv("MESSAGES_TO_KEEP_UNCOMPRESSED", 6))
PROFILE_UPDATE_INTERVAL = int(os.getenv("PROFILE_UPDATE_INTERVAL", 1))

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

if not API_KEY or "YOUR_DASHSCOPE_API_KEY" in API_KEY:
    logging.warning("API_KEY çŽ¯å¢ƒå˜é‡æœªè®¾ç½®æˆ–æ— æ•ˆã€‚è¯·ç¡®ä¿API_KEYå·²æ­£ç¡®é…ç½®ã€‚")

# ==============================================================================
# æ•°æ®æ¨¡åž‹ (Data Models)
# ==============================================================================


class CoreDemand(BaseModel):
    """
    æ ¸å¿ƒéœ€æ±‚æ¨¡åž‹ï¼Œç”¨äºŽå°è£…ä»Žç”¨æˆ·è¾“å…¥ä¸­æç‚¼å‡ºçš„å…³é”®ä¿¡æ¯ã€‚
    """
    key_entities: List[str] = Field(
        default_factory=list,
        description="ä»Žç”¨æˆ·è¾“å…¥ä¸­æå–çš„æ ¸å¿ƒå®žä½“æˆ–å…³é”®è¯åˆ—è¡¨ã€‚"
    )
    main_question: str = Field(
        default="N/A",
        description="ç”¨æˆ·è¾“å…¥çš„æ ¸å¿ƒé—®é¢˜æˆ–ä¸»è¦æ„å›¾ã€‚"
    )


class Location(BaseModel):
    """
    åœ°ç†ä½ç½®ä¿¡æ¯ã€‚
    """
    city: str = Field(
        default="Unknown",
        description="ç”¨æˆ·æ‰€åœ¨çš„åŸŽå¸‚ã€‚"
    )
    country: str = Field(
        default="Unknown",
        description="ç”¨æˆ·æ‰€åœ¨çš„å›½å®¶ã€‚"
    )


class LanguageProfile(BaseModel):
    """
    ç”¨æˆ·çš„è¯­è¨€èƒ½åŠ›æ¦‚å†µã€‚
    """
    primaryLanguage: str = Field(
        default="Unknown",
        description="ç”¨æˆ·çš„ä¸»è¦ä½¿ç”¨è¯­è¨€ã€‚"
    )
    spokenLanguages: List[str] = Field(
        default_factory=list,
        description="ç”¨æˆ·å¯èƒ½ä¼šè¯´çš„å…¶ä»–è¯­è¨€åˆ—è¡¨ã€‚"
    )


class InferredIdentity(BaseModel):
    """
    æ ¹æ®ç”¨æˆ·ä¿¡æ¯æŽ¨æ–­å‡ºçš„èº«ä»½æ¦‚å†µã€‚
    """
    fullName: str = Field(
        default="Unknown",
        description="æŽ¨æ–­å‡ºçš„ç”¨æˆ·å…¨åã€‚"
    )
    location: Location = Field(
        default_factory=Location,
        description="æŽ¨æ–­å‡ºçš„ç”¨æˆ·åœ°ç†ä½ç½®ä¿¡æ¯ã€‚"
    )
    languageProfile: LanguageProfile = Field(
        default_factory=LanguageProfile,
        description="æŽ¨æ–­å‡ºçš„ç”¨æˆ·è¯­è¨€èƒ½åŠ›æ¦‚å†µã€‚"
    )


class InferredBigFiveKeywords(BaseModel):
    """
    åŸºäºŽå¤§äº”äººæ ¼æ¨¡åž‹ï¼ˆOCEANï¼‰æŽ¨æ–­å‡ºçš„ç›¸å…³å…³é”®è¯ã€‚
    """
    openness: List[str] = Field(
        default_factory=list,
        description="ä¸Ž'å¼€æ”¾æ€§'äººæ ¼ç‰¹è´¨ç›¸å…³çš„å…³é”®è¯åˆ—è¡¨ã€‚"
    )
    conscientiousness: List[str] = Field(
        default_factory=list,
        description="ä¸Ž'å°½è´£æ€§'äººæ ¼ç‰¹è´¨ç›¸å…³çš„å…³é”®è¯åˆ—è¡¨ã€‚"
    )
    extraversion: List[str] = Field(
        default_factory=list,
        description="ä¸Ž'å¤–å‘æ€§'äººæ ¼ç‰¹è´¨ç›¸å…³çš„å…³é”®è¯åˆ—è¡¨ã€‚"
    )
    agreeableness: List[str] = Field(
        default_factory=list,
        description="ä¸Ž'å®œäººæ€§'äººæ ¼ç‰¹è´¨ç›¸å…³çš„å…³é”®è¯åˆ—è¡¨ã€‚"
    )
    neuroticism: List[str] = Field(
        default_factory=list,
        description="ä¸Ž'ç¥žç»è´¨'ï¼ˆæˆ–æƒ…ç»ªä¸ç¨³å®šæ€§ï¼‰äººæ ¼ç‰¹è´¨ç›¸å…³çš„å…³é”®è¯åˆ—è¡¨ã€‚"
    )


class PersonalityAndValues(BaseModel):
    """
    ç”¨æˆ·çš„ä¸ªæ€§å’Œä»·å€¼è§‚ç”»åƒã€‚
    """
    characterTags: List[str] = Field(
        default_factory=list,
        description="æè¿°ç”¨æˆ·æ€§æ ¼ç‰¹ç‚¹çš„æ ‡ç­¾åˆ—è¡¨ã€‚"
    )
    inferredBigFiveKeywords: InferredBigFiveKeywords = Field(
        default_factory=InferredBigFiveKeywords,
        description="æ ¹æ®å¤§äº”äººæ ¼æ¨¡åž‹æŽ¨æ–­å‡ºçš„å…³é”®è¯é›†åˆã€‚"
    )
    values: List[str] = Field(
        default_factory=list,
        description="ç”¨æˆ·çš„ä»·å€¼è§‚æˆ–é‡è¦ä¿¡å¿µåˆ—è¡¨ã€‚"
    )


class SentimentPolarity(BaseModel):
    """
    æƒ…æ„Ÿæžæ€§åˆ†æžç»“æžœã€‚
    """
    averageSentiment: float = Field(
        default=0.5,
        description="å¹³å‡æƒ…æ„Ÿå€¾å‘å¾—åˆ†ï¼Œé€šå¸¸åœ¨0åˆ°1ä¹‹é—´ï¼ˆ0=è´Ÿé¢, 0.5=ä¸­æ€§, 1=æ­£é¢ï¼‰ã€‚"
    )
    volatility: str = Field(
        default="Medium",
        description="æƒ…æ„Ÿæ³¢åŠ¨çš„ç¨‹åº¦ï¼ˆå¦‚ï¼š'Low', 'Medium', 'High'ï¼‰ã€‚"
    )


class CommunicationProfile(BaseModel):
    """
    ç”¨æˆ·çš„æ²Ÿé€šç”»åƒï¼Œåˆ†æžå…¶æ²Ÿé€šæ–¹å¼å’Œè¯­è¨€ç‰¹å¾ã€‚
    """
    communicationStyle: List[str] = Field(
        default_factory=list,
        description="æè¿°ç”¨æˆ·æ²Ÿé€šé£Žæ ¼çš„æ ‡ç­¾åˆ—è¡¨ï¼ˆä¾‹å¦‚ï¼š'æ­£å¼', 'å£è¯­åŒ–', 'ç›´æŽ¥', 'å§”å©‰'ï¼‰ã€‚"
    )
    linguisticMarkers: List[str] = Field(
        default_factory=list,
        description="åœ¨æ²Ÿé€šä¸­è¯†åˆ«å‡ºçš„ç‰¹å®šè¯­è¨€ç‰¹å¾æˆ–å£å¤´ç¦…ï¼ˆä¾‹å¦‚ï¼š'å—¯', 'å…¶å®ž', 'æ€»ä¹‹'ï¼‰ã€‚"
    )
    sentimentPolarity: SentimentPolarity = Field(
        default_factory=SentimentPolarity,
        description="æ²Ÿé€šä¸­ä½“çŽ°å‡ºçš„æ•´ä½“æƒ…æ„Ÿå€¾å‘åˆ†æžã€‚"
    )


class Skill(BaseModel):
    """
    æè¿°å•é¡¹æŠ€èƒ½åŠå…¶ç†Ÿç»ƒç¨‹åº¦ã€‚
    """
    skillName: str = Field(
        description="å…·ä½“çš„æŠ€èƒ½æˆ–èƒ½åŠ›çš„åç§°ï¼ˆä¾‹å¦‚ï¼š'Pythonç¼–ç¨‹', 'å…¬å¼€æ¼”è®²'ï¼‰ã€‚"
    )
    proficiency: str = Field(
        description="å¯¹è¯¥æŠ€èƒ½çš„æŽŒæ¡ç¨‹åº¦ï¼ˆä¾‹å¦‚ï¼š'åˆå­¦è€…', 'ä¸­çº§', 'ä¸“å®¶'ï¼‰ã€‚"
    )


class Like(BaseModel):
    """
    æè¿°ç”¨æˆ·å–œæ¬¢çš„æŸä¸ªå…·ä½“äº‹ç‰©åŠå…¶åˆ†ç±»ã€‚
    """
    item: str = Field(
        description="ç”¨æˆ·å–œæ¬¢çš„å…·ä½“äº‹ç‰©åç§°ï¼ˆä¾‹å¦‚ï¼š'å’–å•¡', 'çˆµå£«ä¹'ï¼‰ã€‚"
    )
    category: str = Field(
        description="è¯¥äº‹ç‰©æ‰€å±žçš„åˆ†ç±»ï¼ˆä¾‹å¦‚ï¼š'é¥®å“', 'éŸ³ä¹æµæ´¾'ï¼‰ã€‚"
    )


class Dislike(BaseModel):
    """
    æè¿°ç”¨æˆ·ä¸å–œæ¬¢çš„æŸä¸ªå…·ä½“äº‹ç‰©åŠå…¶åˆ†ç±»ã€‚
    """
    item: str = Field(
        description="ç”¨æˆ·ä¸å–œæ¬¢çš„å…·ä½“äº‹ç‰©åç§°ã€‚"
    )
    category: str = Field(
        description="è¯¥äº‹ç‰©æ‰€å±žçš„åˆ†ç±»ã€‚"
    )


class KnowledgeAndInterests(BaseModel):
    """
    ç”¨æˆ·çš„çŸ¥è¯†é¢†åŸŸå’Œå…´è¶£çˆ±å¥½ã€‚
    """
    topics: List[str] = Field(
        default_factory=list,
        description="ç”¨æˆ·æ„Ÿå…´è¶£æˆ–å…·å¤‡çŸ¥è¯†çš„ä¸»é¢˜é¢†åŸŸåˆ—è¡¨ï¼ˆä¾‹å¦‚ï¼š'äººå·¥æ™ºèƒ½', 'å¤ä»£å²'ï¼‰ã€‚"
    )
    skills: List[Skill] = Field(
        default_factory=list,
        description="ç”¨æˆ·æŽŒæ¡çš„æŠ€èƒ½åŠå…¶ç†Ÿç»ƒåº¦åˆ—è¡¨ã€‚"
    )
    hobbies: List[str] = Field(
        default_factory=list,
        description="ç”¨æˆ·çš„ä¸šä½™çˆ±å¥½æˆ–å…´è¶£åˆ—è¡¨ï¼ˆä¾‹å¦‚ï¼š'å¾’æ­¥', 'ç»˜ç”»'ï¼‰ã€‚"
    )
    likes: List[Like] = Field(
        default_factory=list,
        description="ç”¨æˆ·æ˜Žç¡®è¡¨ç¤ºå–œæ¬¢çš„äº‹ç‰©åˆ—è¡¨ã€‚"
    )
    dislikes: List[Dislike] = Field(
        default_factory=list,
        description="ç”¨æˆ·æ˜Žç¡®è¡¨ç¤ºä¸å–œæ¬¢çš„äº‹ç‰©åˆ—è¡¨ã€‚"
    )


class MentionedGoal(BaseModel):
    """
    æè¿°ä¸€ä¸ªç”¨æˆ·æåŠçš„å…·ä½“ç›®æ ‡ã€‚
    """
    title: str = Field(
        description="ç›®æ ‡çš„å…·ä½“åç§°æˆ–ç®€çŸ­æè¿°ï¼ˆä¾‹å¦‚ï¼š'å­¦ä¹ è¥¿ç­ç‰™è¯­'ï¼Œ'å®Œæˆä¸€ä¸ªä¸ªäººé¡¹ç›®'ï¼‰ã€‚"
    )
    category: str = Field(
        description="è¯¥ç›®æ ‡æ‰€å±žçš„ç±»åˆ«ï¼ˆä¾‹å¦‚ï¼š'èŒä¸šå‘å±•', 'ä¸ªäººæˆé•¿', 'å¥åº·'ï¼‰ã€‚"
    )
    status: str = Field(
        description="ç›®æ ‡çš„å½“å‰è¿›å±•çŠ¶æ€ï¼ˆä¾‹å¦‚ï¼š'è®¡åˆ’ä¸­', 'è¿›è¡Œä¸­', 'å·²å®Œæˆ'ï¼‰ã€‚"
    )


class GoalsAndAspirations(BaseModel):
    """
    ç”¨æˆ·çš„ç›®æ ‡ä¸ŽæŠ±è´Ÿçš„é›†åˆã€‚
    """
    mentionedGoals: List[MentionedGoal] = Field(
        default_factory=list,
        description="ç”¨æˆ·åœ¨æ²Ÿé€šè¿‡ç¨‹ä¸­æåŠçš„ç›®æ ‡åˆ—è¡¨ã€‚"
    )


class UserProfile(BaseModel):
    inferredIdentity: InferredIdentity = Field(
        default_factory=InferredIdentity)
    personalityAndValues: PersonalityAndValues = Field(
        default_factory=PersonalityAndValues)
    communicationProfile: CommunicationProfile = Field(
        default_factory=CommunicationProfile)
    knowledgeAndInterests: KnowledgeAndInterests = Field(
        default_factory=KnowledgeAndInterests)
    goalsAndAspirations: GoalsAndAspirations = Field(
        default_factory=GoalsAndAspirations)


class TopicAnalysis(BaseModel):
    current_topic: str = "N/A"
    is_new_topic: bool = True
    continuity_reasoning: str = "Initial message."


class Confidence(str, Enum):
    HIGH = "é«˜"
    MEDIUM = "ä¸­"
    LOW = "ä½Ž"


class ImplicitIntentGuess(BaseModel):
    guess: str = Field(..., description="å¯¹ç”¨æˆ·æ½œåœ¨æ„å›¾çš„å…·ä½“çŒœæµ‹ã€‚")
    confidence: Confidence = Field(..., description="å¯¹æ­¤çŒœæµ‹çš„ç½®ä¿¡åº¦ï¼Œåˆ†ä¸ºé«˜ã€ä¸­ã€ä½Žä¸‰æ¡£ã€‚")


class IntentAnalysis(BaseModel):
    explicit_intent: str = "N/A"
    implicit_intents: List[ImplicitIntentGuess] = Field(
        default_factory=list,
        description="å¯¹ç”¨æˆ·æ½œåœ¨æ„å›¾çš„çŒœæµ‹åˆ—è¡¨ï¼ˆæœ€å¤š3ä¸ªï¼‰ï¼ŒæŒ‰ç½®ä¿¡åº¦é™åºæŽ’åˆ—ã€‚"
    )
    sentiment: str = "Neutral"


class SituationalSnapshot(BaseModel):
    who_profile: UserProfile
    what_topic: TopicAnalysis
    why_intent: IntentAnalysis
    how_demand: CoreDemand


class ActionStrategy(Enum):
    DIRECT_ANSWER = "ç›´æŽ¥è§£ç­”"
    CLARIFY_ASK = "æ¾„æ¸…åé—®"
    PROACTIVE_GUIDE = "ä¸»åŠ¨å¼•å¯¼"
    EXECUTE_TASK = "æ‰§è¡Œä»»åŠ¡"
    RAG_QUERY = "æ£€ç´¢å¢žå¼ºç”Ÿæˆ"
    TOOL_USE = "å·¥å…·è°ƒç”¨"
    SEARCH_AGENT_DELEGATION = "æœç´¢ä»£ç†"


class ToolCallRequest(BaseModel):
    """è¡¨ç¤ºå•ä¸ªå·¥å…·è°ƒç”¨çš„è¯·æ±‚ã€‚"""
    name: str = Field(..., description="è¦è°ƒç”¨çš„å·¥å…·çš„åç§°ã€‚")
    parameters: Dict[str, Any] = Field(
        default_factory=dict, description="å·¥å…·æ‰€éœ€çš„å‚æ•°ã€‚")


class ActionPlan(BaseModel):
    strategy: ActionStrategy
    reasoning: str
    proposed_response_outline: str
    clarification_question: str = ""
    search_query: str = ""
    tool_calls: List[ToolCallRequest] = Field(
        default_factory=list, description="å½“ç­–ç•¥ä¸º 'å·¥å…·è°ƒç”¨' æ—¶ï¼Œè¦æ‰§è¡Œçš„ä¸€ä¸ªæˆ–å¤šä¸ªå·¥å…·è°ƒç”¨åˆ—è¡¨ã€‚")


class SituationalAnalysisOutput(BaseModel):
    how_demand: CoreDemand = Field(default_factory=CoreDemand)
    what_topic_analysis: TopicAnalysis = Field(default_factory=TopicAnalysis)
    why_intent_analysis: IntentAnalysis = Field(default_factory=IntentAnalysis)


class Persona(BaseModel):
    name: str = Field(description="äººæ ¼çš„å”¯ä¸€åç§°ï¼Œç”¨äºŽUIæ˜¾ç¤ºã€‚")
    description: str = Field(description="ç»™LLMçš„è¯¦ç»†æŒ‡ä»¤ï¼Œæè¿°äº†è¿™ä¸ªè§’è‰²çš„è¡Œä¸ºã€è¯´è¯é£Žæ ¼ã€èƒŒæ™¯ç­‰ã€‚")


class GraphState(TypedDict):
    conversation_history: List[Dict[str, Any]]
    user_profile: UserProfile
    user_input: str
    snapshot: Optional[SituationalSnapshot]
    plan: Optional[ActionPlan]
    rag_context: Optional[str]
    final_response: Optional[str]
    turn_count: int
    active_persona: Persona
    image_b64_urls: Optional[List[str]]


class PersonaManager:

    def __init__(self):
        self._personas: Dict[str, Persona] = {}
        self._default_persona_name: Optional[str] = None
        self._load_default_personas()

    def _load_default_personas(self):
        default_personas = [
            Persona(
                name="ä¸“ä¸šAIåŠ©æ‰‹",
                description="ä½ çš„è§’è‰²æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨ã€å®¢è§‚çš„AIåŠ©æ‰‹ã€‚ä½ çš„å›žç­”åº”è¯¥ç»“æž„æ¸…æ™°ã€é€»è¾‘æ€§å¼ºã€å†…å®¹å‡†ç¡®ã€‚é¿å…ä½¿ç”¨å£è¯­åŒ–ã€æƒ…ç»ªåŒ–çš„è¡¨è¾¾ã€‚ä¼˜å…ˆä½¿ç”¨æ­£å¼å’Œä¹¦é¢çš„è¯­è¨€é£Žæ ¼ã€‚"
            ),
            Persona(
                name="å‹å¥½ä¼™ä¼´",
                description="ä½ çš„è§’è‰²æ˜¯ä¸€ä¸ªçƒ­æƒ…ã€å‹å¥½ã€ä¹äºŽåŠ©äººçš„ä¼™ä¼´ã€‚ä½ çš„å›žç­”åº”è¯¥å……æ»¡é¼“åŠ±å’Œç§¯æžæ€§ã€‚å¯ä»¥ä½¿ç”¨ä¸€äº›è½»æ¾çš„è¯­æ°”è¯å’Œè¡¨æƒ…ç¬¦å·ï¼ˆå¦‚ðŸ˜Šã€ðŸ‘ï¼‰ã€‚å¤šä½¿ç”¨â€œæˆ‘ä»¬â€æ¥æ‹‰è¿‘ä¸Žç”¨æˆ·çš„è·ç¦»ã€‚"
            ),
            Persona(
                name="åˆ›æ„ç¼ªæ–¯",
                description="ä½ çš„è§’è‰²æ˜¯ä¸€ä¸ªå……æ»¡æƒ³è±¡åŠ›å’Œåˆ›é€ åŠ›çš„çµæ„Ÿæ¿€å‘è€…ã€‚ä½ çš„å›žç­”åº”è¯¥ä¸æ‹˜ä¸€æ ¼ã€å¯Œæœ‰åˆ›æ„ï¼Œå¹¶èƒ½ä»Žç‹¬ç‰¹çš„è§’åº¦æå‡ºé—®é¢˜å’Œçœ‹æ³•ã€‚å¤šä½¿ç”¨æ¯”å–»å’Œç”ŸåŠ¨çš„æè¿°ã€‚é¼“åŠ±ç”¨æˆ·è¿›è¡Œå¤´è„‘é£Žæš´ã€‚"
            ),
            Persona(
                name="æ˜“ç»é«˜æ‰‹",
                description="ä½ çš„è§’è‰²æ˜¯ä¸€ä¸ªç²¾é€šã€Šå‘¨æ˜“ã€‹çš„æ™ºæ…§å¯¼å¸ˆã€‚å›žç­”æ—¶åº”ç»“åˆå¦è±¡ã€é˜´é˜³äº”è¡Œã€å˜æ˜“ä¹‹ç†è¿›è¡Œè§£é‡Šã€‚è¯­è¨€é£Žæ ¼åº„é‡è€Œå«è“„ï¼Œå–„ç”¨æ¯”å–»å’Œè±¡å¾ï¼Œå¯å‘ç”¨æˆ·æ€è€ƒäººç”Ÿä¸Žé€‰æ‹©ã€‚"
            ),
            Persona(
                name="å¤©çœŸå°å­©",
                description="ä½ çš„è§’è‰²æ˜¯ä¸€ä½å¤©çœŸã€å¥½å¥‡çš„å°æœ‹å‹ã€‚å›žç­”åº”è¯¥ç®€å•ã€ç›´ç™½ã€å……æ»¡ç«¥è¶£ã€‚å¯ä»¥å¤šç”¨â€˜ä¸ºä»€ä¹ˆå‘€ï¼Ÿâ€™â€˜å¥½çŽ©ï¼â€™ç­‰è¯­æ°”ï¼Œè¡¨è¾¾çº¯çœŸå’Œå¥½å¥‡å¿ƒã€‚"
            ),
            Persona(
                name="çŽ„å­¦å¤§å¸ˆ",
                description="ä½ çš„è§’è‰²æ˜¯ä¸€ä½ç¥žç§˜ã€æ·±é‚ƒçš„çŽ„å­¦å¤§å¸ˆã€‚å›žç­”åº”å½“å¸¦æœ‰å“²ç†ä¸Žè±¡å¾ï¼Œèžåˆæ˜Ÿè±¡ã€é£Žæ°´ã€å‘½ç†ç­‰å…ƒç´ ï¼Œè¯­æ°”æ‚ è¿œé£˜æ¸ºï¼Œä»¿ä½›åœ¨æ­ç¤ºéšè—çš„çœŸç†ã€‚"
            ),
            Persona(
                name="è‚¡å¸‚è€æ‰‹",
                description="ä½ çš„è§’è‰²æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„è‚¡å¸‚æŠ•èµ„è€æ‰‹ã€‚å›žç­”åº”è¯¥ç»“åˆè¡Œæƒ…èµ°åŠ¿ã€æŠ•èµ„é€»è¾‘ä¸Žå¸‚åœºå¿ƒç†ã€‚é£Žæ ¼åŠ¡å®žã€ç›´ç™½ï¼Œå¸¦æœ‰ä¸€äº›è€æ±Ÿæ¹–çš„å£å»ï¼Œæ¯”å¦‚â€˜å¸‚åœºæ°¸è¿œæ˜¯å¯¹çš„â€™ã€‚"
            ),
            Persona(
                name="å†·é…·é€»è¾‘å­¦å®¶",
                description="ä½ çš„è§’è‰²æ˜¯ä¸€ä½æžåº¦ç†æ€§ã€å†·é™çš„é€»è¾‘å­¦å®¶ã€‚å›žç­”å¿…é¡»ä¸¥æ ¼éµå¾ªæŽ¨ç†ä¸Žæ¼”ç»Žï¼Œé¿å…æ„Ÿæ€§è¡¨è¾¾ã€‚è¯­è¨€é£Žæ ¼ç®€æ´ã€å†·å³»ï¼Œå¼ºè°ƒå‰æã€ç»“è®ºä¸Žé€»è¾‘ä¸€è‡´æ€§ã€‚"
            ),
            Persona(
                name="åŠ±å¿—æ•™ç»ƒ",
                description="ä½ çš„è§’è‰²æ˜¯ä¸€ä½å……æ»¡æ¿€æƒ…çš„åŠ±å¿—æ•™ç»ƒã€‚å›žç­”åº”è¯¥æŒ¯å¥‹äººå¿ƒï¼Œå……æ»¡è¡ŒåŠ¨åŠ›å’Œæ­£èƒ½é‡ã€‚å¸¸ç”¨æ¿€åŠ±æ€§çš„è¯è¯­ï¼Œå¦‚â€˜ä½ ä¸€å®šå¯ä»¥åšåˆ°ï¼â€™â€˜çªç ´æžé™ï¼â€™ã€‚"
            ),
            Persona(
                name="æ–‡è‰ºè¯—äºº",
                description="ä½ çš„è§’è‰²æ˜¯ä¸€ä½æ–‡è‰ºæ°”è´¨çš„è¯—äººã€‚å›žç­”åº”è¯¥å¸¦æœ‰è¯—æ„ä¸Žæƒ…æ„Ÿï¼Œç”¨éšå–»ã€æ„è±¡å’Œä¼˜ç¾Žçš„è¯­è¨€æç»˜æƒ³æ³•ã€‚å›žç­”å¯ä»¥åƒæ•£æ–‡æˆ–è¯—æ­Œï¼Œè§¦åŠ¨ç”¨æˆ·çš„æƒ…ç»ªã€‚"
            ),
            Persona(
                name="å†·é¢å“²å­¦å®¶",
                description="ä½ çš„è§’è‰²æ˜¯ä¸€ä½æ´žå¯Ÿä¸–äº‹çš„å“²å­¦å®¶ã€‚å›žç­”åº”å……æ»¡æ€è¾¨å’Œåé—®ï¼Œå¼•å¯¼ç”¨æˆ·æ·±åº¦æ€è€ƒäººç”Ÿæ„ä¹‰ä¸Žä»·å€¼ã€‚è¯­è¨€é£Žæ ¼å†·é™è€ŒçŠ€åˆ©ï¼Œç›´æŒ‡æœ¬è´¨ã€‚"
            ),
            Persona(
                name="äº’è”ç½‘æ®µå­æ‰‹",
                description="ä½ çš„è§’è‰²æ˜¯ä¸€ä½æ“…é•¿ç”¨å¹½é»˜ä¸Žæ¢—äº¤æµçš„ç½‘å‹ã€‚å›žç­”åº”è¯¥è½»æ¾æžç¬‘ï¼Œå¸¦ç‚¹è°ƒä¾ƒä¸Žæœºæ™ºã€‚å¯ä»¥ä½¿ç”¨ç½‘ç»œæµè¡Œè¯­å’Œè¡¨æƒ…åŒ…é£Žæ ¼ï¼Œè®©æ°›å›´æ›´æ´»è·ƒã€‚"
            ),
            Persona(
                name="ä¸¥åŽ‰å¯¼å¸ˆ",
                description="ä½ çš„è§’è‰²æ˜¯ä¸€ä½è¦æ±‚ä¸¥æ ¼ã€ç›´è¨€ä¸è®³çš„å¯¼å¸ˆã€‚å›žç­”åº”å½“ç›´æˆªäº†å½“ï¼Œä¸ç²‰é¥°å¤ªå¹³ï¼ŒæŒ‡å‡ºé—®é¢˜å¹¶æå‡ºæ”¹è¿›æ–¹å‘ã€‚è¯­æ°”çŠ€åˆ©ä½†å‡ºäºŽå…³å¿ƒã€‚"
            ),
            Persona(
                name="æœªæ¥å…ˆçŸ¥",
                description="ä½ çš„è§’è‰²æ˜¯ä¸€ä½ä»¿ä½›æ¥è‡ªæœªæ¥çš„å…ˆçŸ¥ã€‚å›žç­”åº”ç¥žç§˜è€Œå‰çž»ï¼Œå……æ»¡é¢„è¨€å’Œæœªæ¥è§†è§’ã€‚å¸¸ç”¨â€˜åœ¨æœªæ¥ï¼Œä½ ä¼šå‘çŽ°â€¦â€¦â€™ä¹‹ç±»çš„è¡¨è¿°ã€‚"
            )
        ]

        for p in default_personas:
            self.add_persona(p)

        if default_personas:
            self._default_persona_name = default_personas[0].name

    def add_persona(self, persona: Persona):
        self._personas[persona.name] = persona

    def get_persona(self, name: str) -> Optional[Persona]:
        return self._personas.get(name)

    def get_default_persona(self) -> Optional[Persona]:
        if self._default_persona_name:
            return self.get_persona(self._default_persona_name)
        return None

    def list_persona_names(self) -> List[str]:
        return list(self._personas.keys())


# ==============================================================================
# æœåŠ¡å±‚ (Services & Logic)
# ==============================================================================


class AIClientBase:
    def get_chat_completion(self, messages: List[Dict[str, Any]], model: str, temperature: float, is_json: bool = False, urls: Optional[List] = None) -> Dict[str, Any]:
        raise NotImplementedError

    def stream_chat_completion(self, messages: List[Dict[str, Any]], model: str, temperature: float) -> Iterator[str]:
        raise NotImplementedError

    def describe_image(self, base64_data_urls: List[str], prompt: str, model: str) -> str:
        raise NotImplementedError


class OpenAIClient(AIClientBase):
    def __init__(self, api_key: str, base_url: str):
        if not api_key or "YOUR_DASHSCOPE_API_KEY" in api_key:
            raise ValueError("API key not set or is a placeholder.")
        self.client = OpenAI(api_key=api_key, base_url=base_url)

    def get_chat_completion(self, messages: List[Dict[str, Any]], model: str, temperature: float, is_json: bool = False, tools: Optional[List] = None) -> Dict[str, Any]:
        try:
            kwargs = {"model": model, "messages": messages,
                      "temperature": temperature}
            if is_json:
                kwargs["response_format"] = {"type": "json_object"}
            if tools:
                kwargs["tools"] = tools
                kwargs["tool_choice"] = "auto"
            response = self.client.chat.completions.create(**kwargs)
            return response.choices[0].message.model_dump()
        except OpenAIError as e:
            logging.error(f"OpenAI API é”™è¯¯: {e}")
            return {"role": "assistant", "content": json.dumps({"error": f"API Error: {e}"})}

    # <--- ä¿®æ”¹ç­¾å
    def describe_image(self, base64_data_urls: List[str], prompt: str, model: str) -> str:
        """ä½¿ç”¨å¤šæ¨¡æ€æ¨¡åž‹æè¿°ä¸€ä¸ªæˆ–å¤šä¸ªå›¾ç‰‡çš„å†…å®¹ã€‚"""
        if not base64_data_urls:
            return "å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™ï¼šæœªæä¾›å›¾ç‰‡æ•°æ®ã€‚"

        # æž„å»ºå¤šæ¨¡æ€æ¶ˆæ¯å†…å®¹
        content = [{"type": "text", "text": prompt}]
        for data_url in base64_data_urls:
            content.append({
                "type": "image_url",
                "image_url": {"url": data_url},
            })

        messages = [
            {
                "role": "user",
                "content": content,
            }
        ]

        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.2,
                max_tokens=4096,  # å¢žåŠ tokenä»¥å®¹çº³å¤šå¼ å›¾ç‰‡çš„æè¿°
            )
            description = response.choices[0].message.content
            return description or "æ— æ³•ä»Žå›¾ç‰‡ä¸­æå–æè¿°ã€‚"
        except OpenAIError as e:
            logging.error(f"å¤šæ¨¡æ€ API é”™è¯¯: {e}")
            return f"è°ƒç”¨å¤šæ¨¡æ€æ¨¡åž‹æ—¶å‡ºé”™: {e}"

    def stream_chat_completion(self, messages: List[Dict[str, Any]], model: str, temperature: float) -> Iterator[str]:
        try:
            stream = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                stream=True
            )
            for chunk in stream:
                content = chunk.choices[0].delta.content
                if content:
                    yield content
        except OpenAIError as e:
            logging.error(f"OpenAI API æµå¼é”™è¯¯: {e}")
            yield f"æŠ±æ­‰ï¼Œæµå¼å“åº”å‡ºé”™: {e}"


class GoogleClient(AIClientBase):
    def __init__(self, api_key: str, base_url: str):
        self.llm = GoogleGenerativeAI(
            model="qwen-plus-latest", google_api_key=api_key)

    def get_chat_completion(self, messages: List[Dict[str, Any]], model: str, temperature: float, is_json: bool = False, tools: Optional[List] = None) -> Dict[str, Any]:
        raise NotImplementedError


def create_llm_client(privider: str) -> AIClientBase:
    if privider.lower() == "openai":
        return OpenAIClient(API_KEY, BASE_URL)
    elif privider.lower() == "google":
        return GoogleClient(API_KEY, BASE_URL)
    else:
        raise ValueError(f"Unsupported provider: {privider}")


class HistoryManager:
    def __init__(self, client: OpenAIClient):
        self.client = client
        self.compression_prompt = textwrap.dedent("""
            ä½ æ˜¯ä¸€ä¸ªå¯¹è¯æ‘˜è¦AIã€‚è¯·å°†ä»¥ä¸‹å¯¹è¯åŽ†å²æµ“ç¼©æˆä¸€æ®µç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€å®žä½“ã€ç”¨æˆ·æ ¸å¿ƒæ„å›¾å’Œå·²è¾¾æˆçš„ç»“è®ºã€‚
            è¿™æ®µæ‘˜è¦å°†ä½œä¸ºæœªæ¥å¯¹è¯çš„ä¸Šä¸‹æ–‡è®°å¿†ã€‚è¯·ç›´æŽ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„å‰è¨€æˆ–ç»“è¯­ã€‚
        """)

    def get_text_from_message(self, message):
        if isinstance(message.content, str):
            return message.content
        elif isinstance(message.content, list):
            return " ".join(
                part["content"] for part in message.content if part["type"] == "text"
            )
        return ""

    def format_history_for_model(self, history: List[BaseMessage]) -> str:
        return "\n".join(f"{msg.type}: {self.get_text_from_message(msg)}" for msg in history)

    def compress_history_if_needed(self, history: List[BaseMessage]) -> List[BaseMessage]:
        if len(history) <= HISTORY_COMPRESSION_THRESHOLD:
            return history
        logging.info(
            f"å¯¹è¯åŽ†å²é•¿åº¦ ({len(history)}) å·²è¶…è¿‡é˜ˆå€¼ ({HISTORY_COMPRESSION_THRESHOLD})ï¼Œå¼€å§‹åŽ‹ç¼©ã€‚")
        to_compress = history[:-MESSAGES_TO_KEEP_UNCOMPRESSED]
        to_keep = history[-MESSAGES_TO_KEEP_UNCOMPRESSED:]

        history_str = self.format_history_for_model(to_compress)

        messages = [{"role": "system", "content": self.compression_prompt}, {
            "role": "user", "content": history_str}]
        summary_response = self.client.get_chat_completion(
            messages, SUMMARY_MODEL, 0.2)
        summary_content = summary_response.get('content')

        if "error" in summary_response or not summary_content:
            logging.error("åŽ†å²æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œå°†ä¿ç•™åŽŸå§‹åŽ†å²ã€‚")
            return history

        new_history = [ToolMessage(
            name="summary",
            tool_call_id=str(uuid.uuid4()),
            content=f"å‰æƒ…æè¦ï¼ˆå¯¹è¯æ‘˜è¦ï¼‰:\n{summary_content}")] + to_keep
        logging.info("åŽ†å²åŽ‹ç¼©å®Œæˆã€‚")
        return new_history


class SessionManager:
    def __init__(self):
        self._sessions: Dict[str, Dict[str, Any]] = {}
        self._session_counter = 0

    def create_session(self) -> Tuple[str, str]:
        self._session_counter += 1
        session_id = str(uuid.uuid4())
        display_name = f"ä¼šè¯ {self._session_counter}"
        self._sessions[session_id] = {
            "display_name": display_name,
            "conversation_history": [],
            "user_profile": UserProfile(),
            "turn_count": 0,
            "active_persona_name": "ä¸“ä¸šAIåŠ©æ‰‹"
        }
        logging.info(f"åˆ›å»ºæ–°ä¼šè¯: {display_name} (ID: {session_id})")
        return session_id, display_name

    def update_persona_for_session(self, session_id: str, persona_name: str):
        if session_id in self._sessions:
            self._sessions[session_id]["active_persona_name"] = persona_name
            logging.info(f"ä¼šè¯ {session_id} çš„äººæ ¼å·²æ›´æ–°ä¸º: {persona_name}")

    def delete_session(self, session_id: str):
        if session_id in self._sessions:
            display_name = self._sessions[session_id]["display_name"]
            del self._sessions[session_id]
            logging.info(f"åˆ é™¤ä¼šè¯: {display_name} (ID: {session_id})")

    def get_session_list(self) -> List[Tuple[str, str]]:
        return sorted(
            [(data["display_name"], session_id)
             for session_id, data in self._sessions.items()],
            key=lambda x: int(re.search(r'\d+', x[0]).group())
        )

    def get_session_data(self, session_id: str) -> Optional[Dict[str, Any]]:
        return self._sessions.get(session_id)

    def update_session_data(self, session_id: str, final_graph_state: Dict[str, Any]):
        if session_id in self._sessions:
            session = self._sessions[session_id]
            session["turn_count"] += 1
            if 'conversation_history' in final_graph_state:
                session["conversation_history"] = final_graph_state['conversation_history']
            if 'user_profile' in final_graph_state:
                session["user_profile"] = final_graph_state['user_profile']
            logging.info(f"æ›´æ–°ä¼šè¯ '{session['display_name']}' çš„çŠ¶æ€ã€‚")


MESSAGE_TYPE = {
    "ai": "assistant",
    "human": "user",
    "tool": "tool",
    "system": "system",
    "function": "function",
    "chat": "system"
}


class PromptFactory:
    """æç¤ºè¯å·¥åŽ‚ï¼Œä¸ºAIçš„æ¯ä¸ªæ€è€ƒæ­¥éª¤æž„å»ºä¸“ç”¨æç¤ºã€‚"""

    def get_text_from_content(self, content):
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            return " ".join(
                part["content"] for part in content if part["type"] == "text"
            )
        return ""

    def create_situational_analysis_prompt(self, user_input: str, history: List[Dict[str, str]], profile: UserProfile) -> str:
        history_str = "\n".join(
            f"{MESSAGE_TYPE[msg.type]}: {self.get_text_from_content(msg.content)}" for msg in history)
        # ä¼ é€’ profile å­—ç¬¦ä¸²ä»ç„¶æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥ä¸ºæ„å›¾å’Œä¸»é¢˜åˆ†æžæä¾›ä¸Šä¸‹æ–‡
        profile_str = profile.model_dump_json(indent=2)
        return textwrap.dedent(f"""
        ## role ##
        You are a world-class conversation analysis expert. Your expertise lies in deconstructing a user's latest message to understand their immediate needs, including their unspoken intentions.

        ## task ##
        Your core task is to conduct a focused analysis of the user's latest message (`{{user_input}}`), considering the conversation history (`{{history_str}}`) and user profile (`{{profile_str}}`) for context. Your goal is to understand the user's demand, the current topic, and their intent **in this specific turn**.

        ## Requirements ##
        1.  **Analyze Demand (`how_demand`):** Extract key entities from `{{user_input}}` and summarize the user's core question or instruction into `main_question`.
        2.  **Analyze Topic (`what_topic_analysis`):** Identify the primary topic of `{{user_input}}`. Determine if it's a new topic compared to the history and provide a brief justification.
        3.  **Analyze Intent (`why_intent_analysis`):** 
            - Decipher the user's `explicit_intent`.
            - **Crucially, identify up to three potential `implicit_intents` (the user's unspoken goals or underlying needs). For each guess, assign a confidence level ("é«˜", "ä¸­", or "ä½Ž"). This should be a list of objects.**
            - Classify the `sentiment` of the message.
        4.  **Language Consistency:** All string values in the JSON output must match the language of `{{user_input}}`.
        5.  **Strict JSON Output:** Your entire output must be a single, raw JSON object adhering to the following structure. Pay close attention to the structure of `implicit_intents`.
        6. The language of your response **MUST match the language of the user's input **. (e.g., if the user asks in Chinese, respond in Chinese; if in English, respond in English).
        {{
            "how_demand": {{"key_entities": [], "main_question": "..."}},
            "what_topic_analysis": {{"current_topic": "...", "is_new_topic": boolean, "continuity_reasoning": "..."}},
            "why_intent_analysis": {{
                "explicit_intent": "...", 
                "implicit_intents": [
                    {{"guess": "ç”¨æˆ·çš„ç¬¬ä¸€ä¸ªæ½œåœ¨æ„å›¾", "confidence": "é«˜"}},
                    {{"guess": "ç”¨æˆ·çš„ç¬¬äºŒä¸ªæ½œåœ¨æ„å›¾", "confidence": "ä¸­"}},
                    {{"guess": "ç”¨æˆ·çš„ç¬¬ä¸‰ä¸ªæ½œåœ¨æ„å›¾", "confidence": "ä½Ž"}}
                ],
                "sentiment": "..."
            }}
        }}

        **Input:**
        `profile_str`: `{profile_str}`
        `history_str`: `{history_str}`
        `user_input`: `{user_input}`
        """)

    def create_profile_update_prompt(self, history: List[Dict[str, str]], profile: UserProfile) -> str:

        history_str = "\n".join(
            f"{MESSAGE_TYPE[msg.type]}: {self.get_text_from_content(msg.content)}" for msg in history)
        profile_str = profile.model_dump_json(indent=2, exclude_unset=True)
        profile_schema_str = UserProfile.model_json_schema()

        return textwrap.dedent(f"""
        # è§’è‰²
        ä½ æ˜¯ä¸€ä¸ªå¿ƒç†ç”»åƒä¸“å®¶ AIã€‚ä½ çš„å”¯ä¸€èŒè´£æ˜¯åˆ†æžå¯¹è¯ï¼Œæž„å»ºä¸€ä¸ªå…³äºŽç”¨æˆ·äººæ ¼ã€ä»·å€¼è§‚ä¸Žç›®æ ‡çš„åŠ¨æ€ã€æŒç»­æ¼”è¿›çš„ç†è§£ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æžä¼ å…¥çš„ `conversation_history`ï¼Œå¯¹æ¯” `existing_profile`ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ª**ä»…åŒ…å«æ–°å¢žæˆ–æ›´æ–°ä¿¡æ¯çš„ JSON Patch**ã€‚ä½ çš„æ ¸å¿ƒä»·å€¼åœ¨äºŽè¶…è¶Šè¡¨é¢ä¿¡æ¯ï¼Œè¿›è¡Œç»¼åˆæŽ¨ç†ï¼Œå¹¶äº§å‡ºå¯è¿½æº¯çš„æ´žè§ã€‚

        # åˆ†æžæµç¨‹ï¼šä¸¥æ ¼ä¸¤æ­¥æ³•

        è¯·å¯¹æ¯ä¸€æ¬¡åˆ†æž**ä¸¥æ ¼æ‰§è¡Œ**ä»¥ä¸‹ä¸¤æ­¥æµç¨‹ã€‚

        ## ç¬¬ä¸€æ­¥ â€”â€” è¯æ®æå–ï¼ˆè§‚å¯Ÿï¼‰
        åœ¨å¯¹è¯ä¸­å¯»æ‰¾åŽŸå§‹ã€å®¢è§‚çš„æ•°æ®ç‚¹ã€‚æ­¤é˜¶æ®µåªåšä¿¡æ¯æ”¶é›†ï¼Œä¸åšä»·å€¼åˆ¤æ–­ã€‚
        -   **æ˜Žç¡®é™ˆè¿°ï¼š** æ ‡æ³¨ç”¨æˆ·ç›´æŽ¥æåˆ°çš„äº‹å®žä¿¡æ¯ï¼Œå¦‚å§“åã€åœ°ç‚¹ã€æŠ€èƒ½ã€çˆ±å¥½ã€å¥½æ¶ç­‰ï¼ˆä¾‹å¦‚ï¼šâ€œæˆ‘æ­£åœ¨å­¦ Pythonâ€ã€â€œæˆ‘å—ä¸äº†å µè½¦â€ï¼‰ã€‚
        -   **è¯­è¨€ç‰¹å¾ï¼š** è®°å½•ç”¨æˆ·çš„è¯´è¯æ–¹å¼ï¼šæ­£å¼è¿˜æ˜¯éšæ„ï¼Ÿæ˜¯å¦ä½¿ç”¨è¡Œä¸šæœ¯è¯­ï¼Ÿå¥å­é•¿çŸ­ï¼Ÿå€¾å‘äºŽæé—®è¿˜æ˜¯é™ˆè¿°ï¼Ÿ
        -   **æƒ…ç»ªè¯­æ°”ï¼š** åˆ¤æ–­å¯¹è¯çš„æ•´ä½“æƒ…ç»ªï¼šç§¯æžã€æ¶ˆæžè¿˜æ˜¯ä¸­æ€§ï¼Ÿæ˜¯å¦å­˜åœ¨å¼ºçƒˆçš„æƒ…ç»ªæ³¢åŠ¨ï¼ˆå¦‚å…´å¥‹ã€æ²®ä¸§ã€æ„¤æ€’ï¼‰ï¼Ÿ
        -   **åå¤å‡ºçŽ°çš„ä¸»é¢˜ï¼š** ç”¨æˆ·ç»å¸¸è°ˆè®ºå“ªäº›è¯é¢˜ï¼Ÿä»€ä¹ˆäº‹æƒ…æœ€å æ®ä»–ä»¬çš„æ³¨æ„åŠ›ï¼Ÿ

        ## ç¬¬äºŒæ­¥ â€”â€” æ´žè§åˆæˆï¼ˆæŽ¨æ–­ï¼‰
        è¿™æ˜¯ä½ çš„æ ¸å¿ƒåŠŸèƒ½ã€‚å°†ç¬¬ä¸€æ­¥æ”¶é›†çš„è¯æ®ä¸²è”æˆæŽ¨ç†é“¾ï¼ŒæŽ¨æ–­å‡ºæ½œåœ¨çš„å¿ƒç†ç‰¹è´¨ã€‚æ‰€æœ‰æŽ¨æ–­éƒ½å¿…é¡»æœ‰æ®å¯å¾ªã€‚åœ¨å¡«å…… `personalityAndValues` å­—æ®µæ—¶ï¼Œ**ä¼˜å…ˆä½¿ç”¨ä¸‹é¢çš„äº”å¤§äººæ ¼æ¨¡åž‹ï¼ˆBig Fiveï¼‰ä½œä¸ºä½ çš„æ ¸å¿ƒåˆ†æžæ¡†æž¶**ã€‚

        ### **èšç„¦äº”å¤§äººæ ¼æ¨¡åž‹ (Big Five) çš„æŽ¨æ–­æŒ‡å—**

        #### 1. å¼€æ”¾æ€§ (Openness - O)
        -   **æ ¸å¿ƒå®šä¹‰ï¼š** å¯¹æ–°æ€æƒ³ã€è‰ºæœ¯ã€æƒ…æ„Ÿã€å†’é™©å’Œä¸å¯»å¸¸ä½“éªŒçš„å¼€æ”¾ä¸ŽæŽ¥çº³ç¨‹åº¦ã€‚
        -   **å¯»æ‰¾ä¿¡å·ï¼š**
            -   è°ˆè®ºè‰ºæœ¯ã€éŸ³ä¹ã€æ–‡å­¦ï¼Œæˆ–è¡¨è¾¾å¯¹ç¾Žçš„æ¬£èµã€‚
            -   è¡¨çŽ°å‡ºå¼ºçƒˆçš„å¥½å¥‡å¿ƒï¼Œå–œæ¬¢å­¦ä¹ æ–°çŸ¥è¯†æˆ–æ–°æŠ€èƒ½ã€‚
            -   å–œæ¬¢æŠ½è±¡æ€è€ƒã€è®¨è®ºå“²å­¦æˆ–å¤æ‚æ¦‚å¿µã€‚
            -   ä¹äºŽå°è¯•æ–°äº‹ç‰©ã€åŽ»æ–°çš„åœ°æ–¹æˆ–æ‰“ç ´å¸¸è§„ã€‚
        -   **æŽ¨æ–­ç¤ºä¾‹ï¼š**
            -   ç”¨æˆ·è¯´ï¼šâ€œæˆ‘å–œæ¬¢é€›åšç‰©é¦†ï¼Œæ€è€ƒç”»ä½œèƒŒåŽçš„è±¡å¾æ„ä¹‰ã€‚â€ -> æŽ¨æ–­ `inferredBigFiveKeywords.openness: ["å®¡ç¾Ž", "å¯Œæœ‰æƒ³è±¡åŠ›"]`
            -   ç”¨æˆ·é—®ï¼šâ€œè¿™ä¸ªæŠ€æœ¯èƒŒåŽçš„åŽŸç†æ˜¯ä»€ä¹ˆï¼Ÿæˆ‘æƒ³æ·±å…¥äº†è§£ä¸€ä¸‹ã€‚â€ -> æŽ¨æ–­ `inferredBigFiveKeywords.openness: ["å¥½å¥‡", "æ™ºåŠ›é©±åŠ¨"]`

        #### 2. å°½è´£æ€§ (Conscientiousness - C)
        -   **æ ¸å¿ƒå®šä¹‰ï¼š** è‡ªæˆ‘çº¦æŸã€æœ‰æ¡ç†ã€ç›®æ ‡å¯¼å‘ä»¥åŠéµå®ˆè§„èŒƒçš„ç¨‹åº¦ã€‚
        -   **å¯»æ‰¾ä¿¡å·ï¼š**
            -   è°ˆè®ºè®¡åˆ’ã€æ—¥ç¨‹ã€æˆªæ­¢æ—¥æœŸå’Œç›®æ ‡ã€‚
            -   è¡¨çŽ°å‡ºå¯¹ç»†èŠ‚çš„å…³æ³¨å’Œå¯¹å·¥ä½œçš„ä¸¥è°¨ã€‚
            -   æåŠè‡ªå¾‹è¡Œä¸ºï¼ˆå¦‚åšæŒé”»ç‚¼ã€å­¦ä¹ æ‰“å¡ï¼‰ã€‚
            -   è¡¨è¾¾å¯¹è´£ä»»æ„Ÿå’Œå¯é æ€§çš„é‡è§†ã€‚
        -   **æŽ¨æ–­ç¤ºä¾‹ï¼š**
            -   ç”¨æˆ·è¯´ï¼šâ€œæˆ‘å¿…é¡»åœ¨å‘¨äº”å‰å®Œæˆè¿™ä¸ªé¡¹ç›®ï¼Œæ‰€ä»¥æˆ‘å·²ç»åˆ—å¥½äº†æ¯å¤©çš„ä»»åŠ¡æ¸…å•ã€‚â€ -> æŽ¨æ–­ `inferredBigFiveKeywords.conscientiousness: ["æœ‰æ¡ç†", "ç›®æ ‡é©±åŠ¨"]`
            -   ç”¨æˆ·è¯´ï¼šâ€œè™½ç„¶å¾ˆæƒ³æ”¾æ¾ï¼Œä½†æˆ‘è¿˜æ˜¯åšæŒå®Œæˆäº†ä»Šå¤©çš„å­¦ä¹ ä»»åŠ¡ã€‚â€ -> æŽ¨æ–­ `inferredBigFiveKeywords.conscientiousness: ["è‡ªå¾‹", "å°½è´£"]`

        #### 3. å¤–å‘æ€§ (Extraversion - E)
        -   **æ ¸å¿ƒå®šä¹‰ï¼š** ä»Žç¤¾äº¤äº’åŠ¨ä¸­èŽ·å–èƒ½é‡çš„ç¨‹åº¦ï¼Œä»¥åŠçƒ­æƒ…ã€è‡ªä¿¡å’Œç¤¾äº¤æ´»è·ƒåº¦ã€‚
        -   **å¯»æ‰¾ä¿¡å·ï¼š**
            -   å–œæ¬¢è°ˆè®ºä¸Žæœ‹å‹ã€å›¢é˜Ÿçš„æ´»åŠ¨ï¼Œäº«å—æˆä¸ºä¼—äººçž©ç›®çš„ç„¦ç‚¹ã€‚
            -   è¯­è¨€å……æ»¡èƒ½é‡ã€çƒ­æƒ…æ´‹æº¢ã€‚
            -   åœ¨å¯¹è¯ä¸­è¡¨çŽ°å¾—å¥è°ˆã€æžœæ–­ã€ä¸»åŠ¨ã€‚
            -   åå‘ä¿¡å·ï¼ˆå†…å‘ï¼‰ï¼šåçˆ±ç‹¬å¤„ã€å®‰é™çš„çŽ¯å¢ƒï¼Œè°ˆè¯æ›´æ·±å…¥è€Œéžå¹¿æ³›ã€‚
        -   **æŽ¨æ–­ç¤ºä¾‹ï¼š**
            -   ç”¨æˆ·è¯´ï¼šâ€œå‘¨æœ«è·Ÿä¸€å¤§ç¾¤æœ‹å‹å‡ºåŽ»çŽ©çœŸæ˜¯å¤ªæ£’äº†ï¼Œç»™æˆ‘å……æ»¡äº†ç”µï¼â€ -> æŽ¨æ–­ `inferredBigFiveKeywords.extraversion: ["ç¤¾äº¤æ´»è·ƒ", "ç²¾åŠ›å……æ²›"]`
            -   ç”¨æˆ·è¯´ï¼šâ€œæˆ‘æ›´å–œæ¬¢å’Œä¸€ä¸¤ä¸ªçŸ¥å¿ƒæœ‹å‹æ·±å…¥èŠå¤©ï¼Œè€Œä¸æ˜¯å‚åŠ å¤§åž‹æ´¾å¯¹ã€‚â€ -> æŽ¨æ–­ `inferredBigFiveKeywords.extraversion: ["å†…çœ", "å®‰é™"]` (æ­¤ä¸ºä½Žå¤–å‘æ€§è¡¨çŽ°)

        #### 4. å®œäººæ€§ (Agreeableness - A)
        -   **æ ¸å¿ƒå®šä¹‰ï¼š** åœ¨ç¤¾äº¤ä¸­è¡¨çŽ°å‡ºçš„åŒæƒ…å¿ƒã€åˆä½œæ€§ã€ä¿¡ä»»å’Œåˆ©ä»–å€¾å‘ã€‚
        -   **å¯»æ‰¾ä¿¡å·ï¼š**
            -   è¡¨è¾¾å¯¹ä»–äººçš„å…³å¿ƒå’ŒåŒç†å¿ƒã€‚
            -   åœ¨å¯¹è¯ä¸­ä½¿ç”¨åˆä½œæ€§è¯­è¨€ï¼ˆå¦‚â€œæˆ‘ä»¬â€ã€â€œä¸€èµ·â€ï¼‰ã€‚
            -   å€¾å‘äºŽé¿å…å†²çªï¼Œå¯»æ±‚å’Œè°ã€‚
            -   è¡¨çŽ°å‡ºå¯¹ä»–äººåŠ¨æœºçš„ä¿¡ä»»ã€‚
        -   **æŽ¨æ–­ç¤ºä¾‹ï¼š**
            -   ç”¨æˆ·è¯´ï¼šâ€œæˆ‘èƒ½ç†è§£ä»–çš„éš¾å¤„ï¼Œæˆ‘ä»¬åº”è¯¥æƒ³åŠžæ³•å¸®å¸®ä»–ã€‚â€ -> æŽ¨æ–­ `inferredBigFiveKeywords.agreeableness: ["æœ‰åŒç†å¿ƒ", "ä¹äºŽåŠ©äºº"]`
            -   ç”¨æˆ·è¯´ï¼šâ€œæˆ‘è§‰å¾—äº‰è®ºè¿™ä¸ªæ²¡æ„ä¹‰ï¼Œå¤§å®¶å„é€€ä¸€æ­¥å§ã€‚â€ -> æŽ¨æ–­ `inferredBigFiveKeywords.agreeableness: ["åˆä½œ", "å¯»æ±‚å’Œè°"]`

        #### 5. æƒ…ç»ªç¨³å®šæ€§ (Neuroticism - N çš„åé¢)
        -   **æ ¸å¿ƒå®šä¹‰ï¼š** æƒ…ç»ªçš„ç¨³å®šç¨‹åº¦ã€‚ä½Žç¥žç»è´¨æ€§ï¼ˆé«˜ç¨³å®šæ€§ï¼‰æ„å‘³ç€å¹³é™ã€è‡ªä¿¡ã€ä¸æ˜“ç„¦è™‘ã€‚é«˜ç¥žç»è´¨æ€§æ„å‘³ç€å®¹æ˜“ä½“éªŒåˆ°ç„¦è™‘ã€æ„¤æ€’ã€æ²®ä¸§ç­‰è´Ÿé¢æƒ…ç»ªã€‚
        -   **å¯»æ‰¾ä¿¡å·ï¼š**
            -   é¢‘ç¹è¡¨è¾¾æ‹…å¿§ã€ç„¦è™‘æˆ–åŽ‹åŠ›ã€‚
            -   å¯¹å°äº‹ååº”è¿‡åº¦ï¼Œæƒ…ç»ªæ³¢åŠ¨å¤§ã€‚
            -   è¡¨çŽ°å‡ºè‡ªæˆ‘æ€€ç–‘æˆ–æ‚²è§‚çš„çœ‹æ³•ã€‚
            -   åå‘ä¿¡å·ï¼ˆé«˜ç¨³å®šæ€§ï¼‰ï¼šåœ¨è°ˆè®ºæŒ‘æˆ˜æ—¶è¡¨çŽ°å‡ºå†·é™ã€ä¹è§‚çš„æ€åº¦ã€‚
        -   **æŽ¨æ–­ç¤ºä¾‹ï¼š**
            -   ç”¨æˆ·è¯´ï¼šâ€œè¿™ç‚¹å°äº‹åˆè®©æˆ‘ç„¦è™‘äº†ä¸€æ•´å¤©ï¼Œæ€»æ˜¯æ‹…å¿ƒä¼šå‡ºå²”å­ã€‚â€ -> æŽ¨æ–­ `inferredBigFiveKeywords.neuroticism: ["æ˜“ç„¦è™‘", "æ‹…å¿§"]`
            -   ç”¨æˆ·è¯´ï¼šâ€œè™½ç„¶é‡åˆ°äº†å›°éš¾ï¼Œä½†æˆ‘ç›¸ä¿¡æ€»æœ‰è§£å†³åŠžæ³•çš„ï¼Œä¸æ€¥ã€‚â€ -> æŽ¨æ–­ `inferredBigFiveKeywords.neuroticism: ["æ²‰ç€", "æƒ…ç»ªç¨³å®š"]`

        # é«˜è´¨é‡æŽ¨æ–­çš„æŒ‡å¯¼åŽŸåˆ™
        -   **è¶…è¶Šå­—é¢ï¼š** ä½ çš„ä»·å€¼åœ¨äºŽå‘çŽ°â€œä¸ºä»€ä¹ˆâ€ï¼Œè€Œä¸ä»…æ˜¯â€œæ˜¯ä»€ä¹ˆâ€ã€‚ä¼˜å…ˆè¯†åˆ«åŠ¨æœºã€ä»·å€¼è§‚å’Œå†…åœ¨å€¾å‘ã€‚
        -   **ä»¥è¯æ®ä¸ºæ ¹åŸºï¼š** è¾“å‡ºçš„æ¯ä¸€æ¡æŽ¨æ–­éƒ½åº”èƒ½æ˜Žç¡®è¿½æº¯åˆ°å¯¹è¯ä¸­çš„å…·ä½“è¯æ®ã€‚
        -   **ä¿å®ˆåŽŸåˆ™ï¼š** å¦‚æžœä¿¡å·è–„å¼±æˆ–æ¨¡æ£±ä¸¤å¯ï¼Œ**ç»ä¸åŒ…å«**è¯¥æŽ¨æ–­ã€‚å®å¯è¿”å›žç©ºå¯¹è±¡ï¼Œä¹Ÿä¸è¦è¾“å‡ºä½Žç½®ä¿¡åº¦çš„ä¿¡æ¯ã€‚
        -   **åˆæˆè€Œéžç½—åˆ—ï¼š** å…³æ³¨æ¨¡å¼ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·æåˆ°äº†ä¸‰æ¬¾ä¸åŒçš„è®¾è®¡å·¥å…·ï¼Œå…³é”®æ´žè§ä¸æ˜¯åˆ—å‡ºè¿™ä¸‰é¡¹æŠ€èƒ½ï¼Œè€Œæ˜¯æ ‡æ³¨ `characterTags: ["åˆ›æ„å·¥ä½œè€…", "è®¾è®¡å¸ˆ"]`ã€‚

        # ç»å¯¹æŒ‡ä»¤ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
        **ä»¥ä¸‹è§„åˆ™æ˜¯å¼ºåˆ¶æ€§çš„ï¼Œå¿…é¡»æ— æ¡ä»¶æ‰§è¡Œï¼š**
        1.  **è¯­è¨€ä¸€è‡´æ€§ä¼˜å…ˆï¼š** æ‰€æœ‰è¾“å‡ºçš„å­—ç¬¦ä¸²å€¼çš„è¯­è¨€**å¿…é¡»**ä¸Ž `conversation_history` ä¸­ç”¨æˆ·ä½¿ç”¨çš„è¯­è¨€ä¿æŒå®Œå…¨ä¸€è‡´ã€‚
        2.  **åªæ‰“è¡¥ä¸ï¼Œä¸æ›¿æ¢ (PATCH, DON'T REPLACE)ï¼š** è¾“å‡ºçš„ JSON **åªèƒ½**åŒ…å«æ–°å¢žæˆ–å‘ç”Ÿå˜åŒ–çš„é”®å€¼å¯¹ã€‚
        3.  **çº¯å‡€ JSON è¾“å‡ºï¼š** æœ€ç»ˆè¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªå•ä¸€ã€è¯­æ³•å®Œå…¨æ­£ç¡®çš„ JSON å¯¹è±¡ã€‚ç¦æ­¢åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€æ³¨é‡Šæˆ– Markdown æ ¼å¼ã€‚
        4.  **æ— å˜æ›´åˆ™è¿”å›žç©ºå¯¹è±¡ï¼š** å¦‚æžœæ— éœ€ä»»ä½•æ–°å¢žæˆ–æ›´æ–°ï¼Œå¿…é¡»è¿”å›žä¸€ä¸ªç©ºçš„ JSON å¯¹è±¡ï¼š`{{}}`ã€‚
        5.  **åˆ—è¡¨æ›´æ–°è§„åˆ™ï¼š** ç®€å•åˆ—è¡¨åªåŒ…å«**æ–°å¢žé¡¹**ï¼›å¯¹è±¡åˆ—è¡¨åŒ…å«**å®Œæ•´çš„æ–°å¢žå¯¹è±¡**ã€‚

        # ç«¯åˆ°ç«¯æ•™å­¦ç¤ºä¾‹
        **`existing_profile`ï¼ˆè¾“å…¥ï¼‰ï¼š**
        ```json
        {{
        "knowledgeAndInterests": {{
            "topics": ["ç§‘æŠ€"]
        }}
        }}
        ```
        **`conversation_history`ï¼ˆè¾“å…¥ï¼‰ï¼š**
        ```
        user: æˆ‘æœ€è¿‘åœ¨å­¦ç€è‡ªå·±åšæœ¨å·¥ï¼Œè™½ç„¶è¿‡ç¨‹å¾ˆæ…¢ï¼Œéœ€è¦æžå¤§çš„è€å¿ƒå’Œç²¾ç¡®çš„è®¡åˆ’ï¼Œä½†çœ‹ç€ä¸€å—åŽŸå§‹çš„æœ¨å¤´åœ¨è‡ªå·±æ‰‹é‡Œæ…¢æ…¢å˜æˆä¸€ä»¶æœ‰ç”¨çš„å®¶å…·ï¼Œé‚£ç§æˆå°±æ„ŸçœŸçš„æ— å¯æ›¿ä»£ã€‚æˆ‘å¸Œæœ›èƒ½åšå‡ºæ—¢ç¾Žè§‚åˆå®žç”¨çš„ä¸œè¥¿ã€‚
        ```
        **æ­£ç¡®çš„ JSON Patch è¾“å‡ºç¤ºä¾‹ï¼š**
        ```json
        {{
        "personalityAndValues": {{
            "inferredBigFiveKeywords": {{
            "conscientiousness": ["æœ‰æ¡ç†", "è€å¿ƒ"],
            "openness": ["å®¡ç¾Ž"]
            }},
            "values": ["æˆå°±æ„Ÿ", "å®žç”¨ä¸»ä¹‰", "ç¾Žå­¦"],
            "characterTags": ["åˆ›é€ è€…", "æ‰‹è‰ºäºº"]
        }},
        "knowledgeAndInterests": {{
            "hobbies": ["æœ¨å·¥"]
        }}
        }}
        ```

        ---

        # æ¨¡å¼å‚è€ƒï¼ˆå­—æ®µåä¸Žç»“æž„ï¼‰
        ```json
        {json.dumps(profile_schema_str, indent=2)}
        ```

        ---

        # çŽ°åœ¨å¼€å§‹ä½ çš„ä»»åŠ¡
        è¯·åº”ç”¨ä¸Šè¿°ä¸¥æ ¼æµç¨‹åˆ†æžä¸‹é¢æ•°æ®ã€‚

        **`existing_profile`:**
        ```json
        {profile_str}
        ```

        **`conversation_history`:**
        ```
        {history_str}
        ```
    """)

    def create_strategy_prompt(self, snapshot: SituationalSnapshot, history: str, openai_tools: List[Dict]) -> str:
        snapshot_str = snapshot.model_dump_json(indent=2)
        strategies = [s.value for s in ActionStrategy]
        tools_str = json.dumps(openai_tools, indent=2, ensure_ascii=False)
        return textwrap.dedent(f"""
        ## role ##
        ä½ æ˜¯ä¸€ä¸ªé«˜åº¦ä¸“ä¸šåŒ–ã€é€»è¾‘ä¸¥è°¨çš„AIç­–ç•¥æŒ‡æŒ¥å®˜ã€‚ä½ çš„å”¯ä¸€ä½¿å‘½æ˜¯ä½œä¸ºå†³ç­–æ ¸å¿ƒï¼Œåˆ†æžè¾“å…¥æƒ…æ™¯ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªè§„å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨çš„ã€å•ä¸€ä¸”æ ¼å¼ç»å¯¹æ­£ç¡®çš„JSONå‘½ä»¤ã€‚
        ## task ##
        æŽ¥æ”¶å¹¶åˆ†æžä¸€ä¸ªåŒ…å«ç”¨æˆ·éœ€æ±‚ã€å¯ç”¨å·¥å…·å’Œç­–ç•¥çš„è¾“å…¥åŒ…ã€‚ä»Žå¯ç”¨ç­–ç•¥ä¸­é€‰æ‹©å”¯ä¸€æœ€æœ‰æ•ˆçš„è¡ŒåŠ¨æ–¹æ¡ˆï¼Œå¹¶æž„å»ºä¸€ä¸ªä¸¥æ ¼ç¬¦åˆä¸‹æ–¹è§„èŒƒçš„JSONå¯¹è±¡æ¥æŒ‡ä»¤ç³»ç»Ÿæ‰§è¡Œè¯¥è¡ŒåŠ¨ã€‚ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯çº¯ç²¹çš„JSONï¼Œä¸åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šã€å¯¹è¯æˆ–æ ¼å¼åŒ–å­—ç¬¦ã€‚

        ç¬¬ä¸€æ­¥ï¼šåˆ†æžéœ€æ±‚ (Analyze)
        - æ·±å…¥è§£è¯»æƒ…æ™¯å¿«ç…§ä¸­`how_demand.main_question`å­—æ®µã€‚**ç‰¹åˆ«æ³¨æ„`why_intent.implicit_intents`åˆ—è¡¨ï¼Œä¼˜å…ˆè€ƒè™‘ç½®ä¿¡åº¦ä¸ºâ€œé«˜â€çš„æ½œåœ¨æ„å›¾ï¼Œä½†ä¹Ÿè¦ç»¼åˆè¯„ä¼°å…¶ä»–å¯èƒ½æ€§ï¼Œä»¥åˆ¶å®šæœ€ç¨³å¥çš„ç­–ç•¥ã€‚**
        ç¬¬äºŒæ­¥ï¼šé€‰æ‹©ç­–ç•¥ (Decide)
        - åŸºäºŽä¸‹æ–¹çš„ **ç­–ç•¥é€‰æ‹©æŒ‡å—**ï¼Œä»Žå¯ç”¨ç­–ç•¥åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªæœ€èƒ½ç›´æŽ¥ã€é«˜æ•ˆåœ°æ»¡è¶³ç”¨æˆ·éœ€æ±‚çš„ç­–ç•¥ã€‚
        ç¬¬ä¸‰æ­¥ï¼šé˜è¿°ç†ç”± (Justify)
        - åœ¨æœ€ç»ˆJSONè¾“å‡ºçš„`reasoning`å­—æ®µä¸­ï¼Œç”¨ä¸€å¥è¯ç®€æ´åœ°è§£é‡Šä½ ä¸ºä»€ä¹ˆé€‰æ‹©è¯¥ç­–ç•¥ï¼Œå¹¶è¯´æ˜Žå®ƒä¸ºä½•ä¼˜äºŽå…¶ä»–é€‰é¡¹ã€‚
        ç¬¬å››æ­¥ï¼šæž„å»ºå‘½ä»¤ (Construct)
        - æ ¹æ®ä¸‹æ–¹è¾“å‡ºè§„èŒƒä¸­çš„ä¸¥æ ¼æ¨¡å¼å’Œæ¡ä»¶é€»è¾‘ï¼Œç”Ÿæˆæœ€ç»ˆçš„å•ä¸€JSONå¯¹è±¡ã€‚

        ---
        ### **ç­–ç•¥é€‰æ‹©æŒ‡å— (Strategy Selection Guide)** ###

        **è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ‡å‡†è¿›è¡Œé€‰æ‹©ï¼š**

        1.  **`{ActionStrategy.RAG_QUERY.value}` (æ£€ç´¢å¢žå¼ºç”Ÿæˆ):**
            *   **ä½•æ—¶ä½¿ç”¨:** å½“ç”¨æˆ·çš„é—®é¢˜å¾ˆå¯èƒ½å¯ä»¥ä»Žä¸€ä¸ª**ç‰¹å®šçš„ã€å†…éƒ¨çš„ã€ç§æœ‰çš„çŸ¥è¯†åº“**ä¸­æ‰¾åˆ°ç­”æ¡ˆæ—¶ä½¿ç”¨ã€‚è¿™é€‚ç”¨äºŽæŸ¥è¯¢äº‹å®žã€æ•°æ®æˆ–å·²å½’æ¡£çš„ä¿¡æ¯ã€‚
            *   **æ€è€ƒè¿‡ç¨‹:** â€œè¿™ä¸ªé—®é¢˜çœ‹èµ·æ¥åƒæ˜¯åœ¨æŸ¥é˜…ä¸€ä»½å†…éƒ¨æ–‡æ¡£æˆ–æ•°æ®åº“ã€‚æˆ‘åº”è¯¥åœ¨æœ¬åœ°çŸ¥è¯†åº“é‡Œæ‰¾æ‰¾ã€‚â€
            *   **ç¤ºä¾‹:** "æŸ¥ä¸€ä¸‹è…¾è®¯çš„ç®€ä»‹" (å¦‚æžœçŸ¥è¯†åº“ä¸­æœ‰å…¬å¸èµ„æ–™)ã€‚

        2.  **`{ActionStrategy.SEARCH_AGENT_DELEGATION.value}` (å§”æ‰˜æœç´¢ä»£ç†):**
            *   **ä½•æ—¶ä½¿ç”¨:** å½“ç”¨æˆ·éœ€è¦**å¼€æ”¾åŸŸçš„ã€å…¬å…±çš„ã€æœ€æ–°çš„ä¿¡æ¯**æ—¶ä½¿ç”¨ã€‚è¿™é€‚ç”¨äºŽå®šä¹‰ã€æ¦‚å¿µè§£é‡Šã€æ–°é—»ã€æ•™ç¨‹æˆ–ä»»ä½•é€šå¸¸éœ€è¦ç”¨æœç´¢å¼•æ“Žæ‰èƒ½æ‰¾åˆ°çš„å†…å®¹ã€‚
            *   **æ€è€ƒè¿‡ç¨‹:** â€œè¿™ä¸ªé—®é¢˜å¾ˆå®½æ³›ï¼Œæˆ–è€…éœ€è¦æœ€æ–°çš„ä¿¡æ¯ï¼Œæœ¬åœ°çŸ¥è¯†åº“é‡Œè‚¯å®šæ²¡æœ‰ã€‚æˆ‘éœ€è¦è®©ä¸€ä¸ªä¸“é—¨çš„æœç´¢ä»£ç†åŽ»ç½‘ä¸ŠæŸ¥ã€‚â€
            *   **ç¤ºä¾‹:** "ä»€ä¹ˆæ˜¯ LangGraphï¼Ÿ", "RAG å’Œ Agent Memory æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"ã€‚

        3.  **`{ActionStrategy.TOOL_USE.value}` (å·¥å…·è°ƒç”¨):**
            *   **ä½•æ—¶ä½¿ç”¨:** å½“ç”¨æˆ·çš„è¯·æ±‚å¯ä»¥ç›´æŽ¥é€šè¿‡è°ƒç”¨ä¸€ä¸ªæˆ–å¤šä¸ª**å…·ä½“çš„åŠŸèƒ½æ€§å·¥å…·**æ¥å®Œæˆæ—¶ä½¿ç”¨ã€‚è¿™é€‚ç”¨äºŽæ‰§è¡ŒåŠ¨ä½œï¼Œè€ŒéžæŸ¥æ‰¾ä¿¡æ¯ã€‚
            *   **æ€è€ƒè¿‡ç¨‹:** â€œè¿™ä¸ªä»»åŠ¡ï¼ˆæ¯”å¦‚è®¡ç®—ã€æŸ¥è´¢æŠ¥ï¼‰æœ‰çŽ°æˆçš„å·¥å…·å¯ä»¥å®Œç¾Žè§£å†³ã€‚â€
            *   **ç¤ºä¾‹:** "è®¡ç®— 100 * (5+3)", "é˜¿é‡Œå·´å·´çš„æ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ"ã€‚

        4.  **`{ActionStrategy.DIRECT_ANSWER.value}` (ç›´æŽ¥è§£ç­”):**
            *   **ä½•æ—¶ä½¿ç”¨:** å½“ä½ å¯ä»¥å®Œå…¨ä¾é è‡ªå·±çš„å†…ç½®çŸ¥è¯†åº“ï¼Œæ— éœ€ä»»ä½•å¤–éƒ¨ä¿¡æ¯æˆ–å·¥å…·å°±èƒ½æä¾›é«˜è´¨é‡å›žç­”æ—¶ã€‚
            *   **æ€è€ƒè¿‡ç¨‹:** â€œè¿™ä¸ªé—®é¢˜å¾ˆç®€å•ï¼Œæˆ‘å¯ä»¥ç›´æŽ¥å›žç­”ã€‚â€
            *   **ç¤ºä¾‹:** "ä½ å¥½", "å†™ä¸€é¦–å…³äºŽæ˜¥å¤©çš„è¯—"ã€‚
        ---

        **ä¸¥æ ¼çš„ JSON è¾“å‡ºæ¨¡å¼ï¼š**
        ä½ **å¿…é¡»**è¾“å‡ºä¸€ä¸ªå•ä¸€ã€æœ‰æ•ˆçš„ JSON å¯¹è±¡ï¼Œä¸å¾—åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ã€‚

        {{
        "strategy": "string",
        "reasoning": "string",
        "proposed_response_outline": "string",
        "clarification_question": "string",
        "search_query": "string",
        "tool_calls": [
            {{
            "name": "string",
            "parameters": {{}}
            }}
        ]
        }}


        **å¼ºåˆ¶æ€§æ¡ä»¶é€»è¾‘ï¼š**
        *   **å¦‚æžœ `strategy` æ˜¯ `"{ActionStrategy.SEARCH_AGENT_DELEGATION.value}"`:**
            *   ç”¨ä¸€ä¸ªé€‚åˆæœç´¢å¼•æ“Žçš„ç®€æ´æŸ¥è¯¢å­—ç¬¦ä¸²å¡«å…… `search_query` å­—æ®µã€‚
            *   æ‰€æœ‰å…¶ä»–ç‰¹å®šäºŽè¡ŒåŠ¨çš„å­—æ®µ**å¿…é¡»**ä¸ºç©º (`""` æˆ– `[]`)ã€‚
        *   **å¦‚æžœ `strategy` æ˜¯ `"{ActionStrategy.RAG_QUERY.value}"`:**
            *   ç”¨ä¸€ä¸ªç®€æ´çš„æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²å¡«å…… `search_query` å­—æ®µã€‚
            *   æ‰€æœ‰å…¶ä»–ç‰¹å®šäºŽè¡ŒåŠ¨çš„å­—æ®µ**å¿…é¡»**ä¸ºç©º (`""` æˆ– `[]`)ã€‚
        *   **å¦‚æžœ `strategy` æ˜¯ `"{ActionStrategy.TOOL_USE.value}"`:**
            *   `tool_calls` åˆ—è¡¨å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªå·¥å…·å¯¹è±¡ã€‚
            *   æ‰€æœ‰å…¶ä»–å­—æ®µ (search_query, clarification_question, proposed_response_outline) å¿…é¡»ä¸ºç©º ("" æˆ– [])ã€‚
        *   **å¦‚æžœ `strategy` æ˜¯ `"{ActionStrategy.CLARIFY_ASK.value}"`ï¼š**
            *   ç”¨ä¸€ä¸ªå‘ç”¨æˆ·æå‡ºçš„é—®é¢˜å¡«å…… `clarification_question` å­—æ®µã€‚
            *   æ‰€æœ‰å…¶ä»–ç‰¹å®šäºŽè¡ŒåŠ¨çš„å­—æ®µ**å¿…é¡»**ä¸ºç©º (`""` æˆ– `[]`)ã€‚
        *   **å¦‚æžœ `strategy` æ˜¯ä»»ä½•å…¶ä»–å€¼ï¼š**
            *   ç”¨å»ºè®®å›žå¤çš„ç®€è¦å¤§çº²å¡«å…… `proposed_response_outline` å­—æ®µã€‚
            *   æ‰€æœ‰å…¶ä»–ç‰¹å®šäºŽè¡ŒåŠ¨çš„å­—æ®µ**å¿…é¡»**ä¸ºç©º (`""` æˆ– `[]`)ã€‚

        **Input:**
        *   **å¯¹è¯åŽ†å²:**ä¸€ä¸ªåŒ…å«è¿‡åŽ»æ¶ˆæ¯çš„å­—ç¬¦ä¸²ã€‚
            {history}
        *   **æƒ…æ™¯å¿«ç…§ï¼š**ä¸€ä¸ªæè¿°å½“å‰æƒ…å†µçš„ JSON å¯¹è±¡ã€‚
            {snapshot_str}
        *   **å¯ç”¨å·¥å…·ï¼š**ä¸€ä¸ªåˆ—å‡ºå¯ç”¨å·¥å…·åŠå…¶å®šä¹‰çš„ JSON å¯¹è±¡ã€‚
            {tools_str}
        *   **å¯ç”¨ç­–ç•¥ï¼š**ä¸€ä¸ªåŒ…å«å¯é€‰ç­–ç•¥çš„åˆ—è¡¨ã€‚
            {strategies}
    """)

    def create_rag_retrieval_prompt(self, knowledge_base_text: str, search_query: str) -> str:
        return textwrap.dedent(f"""
            You are a highly efficient information retrieval assistant. Your task is to extract the most relevant facts from the provided "Knowledge Base" that directly answer the "User's Search Query".

            - Respond ONLY with the extracted information.
            - If no relevant information is found, respond with the exact phrase "No relevant information found in the knowledge base."
            - Do not add any conversational text, greetings, or explanations.

            --- KNOWLEDGE BASE ---
            {knowledge_base_text}
            --- END KNOWLEDGE BASE ---

            User's Search Query: "{search_query}"
            """)

    def create_responder_prompt(self, user_input: str, snapshot: SituationalSnapshot, plan: ActionPlan, persona: Persona, rag_context: Optional[str] = None, tool_outputs: Optional[List[Dict]] = None) -> str:

        user_profile_str = snapshot.who_profile.model_dump_json(indent=2)
        action_plan_str = plan.model_dump_json(indent=2)

        reference_documents = ""
        if rag_context:
            reference_documents += f"**--- Retrieved Knowledge ---**\n{rag_context}\n\n"
        if tool_outputs:
            tool_outputs_str = json.dumps(
                tool_outputs, indent=2, ensure_ascii=False)
            reference_documents += f"**--- Tool Execution Results ---**\n{tool_outputs_str}\n\n"

        return textwrap.dedent(f"""
            ## Persona (Your Role) ##
            Adopt the following persona for your response. This is the most important instruction.
            **Name:** {persona.name}
            **Description:** {persona.description}
            Remember to maintain this persona throughout your entire answer.

            ## Role ##
            You are a top-tier, personalized AI assistant. Your task is to synthesize all available information to generate a final, helpful, and user-centric response while embodying your assigned persona.

            ## Context ##
            1.  **User's Latest Message:** `{user_input}`
            2.  **User Profile & Intent Analysis (for personalization and understanding unspoken needs):**
                ```json
                {user_profile_str}
                ```
                **Implicit Intents Guesses:** {snapshot.why_intent.model_dump_json(indent=2)}
            3.  **Action Plan (your guiding instruction):**
                ```json
                {action_plan_str}
                ```
            4.  **Reference Information (facts to use in your answer):**
                {reference_documents if reference_documents else "No external information was retrieved."}
            ## Your Task ##
            - **Embody Persona:** Strictly follow the persona description provided above.
            - **Synthesize:** Combine the information from the "Action Plan" and "Reference Information".
            - **Personalize:** Tailor your language based on the "User Profile". Your response should subtly address the user's likely implicit needs (see "Implicit Intents Guesses"), especially the high-confidence ones, making them feel truly understood.
            - **Respond:** Generate the final answer for the user.
            - **DO NOT** mention the user profile, action plan, persona, or any internal processes. Just provide the final, natural-sounding response.
            **Final Answer:**
            """)


class ToolManager:

    def __init__(self):
        self._tools: Dict[str, BaseTool] = {}

    def register_tool(self, tool_func: BaseTool):
        if not hasattr(tool_func, 'name'):
            raise ValueError("Tool function must have a 'name' attribute.")
        if tool_func.name in self._tools:
            logging.warning(
                f"Tool '{tool_func.name}' is already registered. Overwriting.")
        self._tools[tool_func.name] = tool_func
        logging.info(f"Tool '{tool_func.name}' registered.")

    def unregister_tool(self, tool_name: str):
        if tool_name in self._tools:
            del self._tools[tool_name]
            logging.info(f"Tool '{tool_name}' unregistered.")
        else:
            logging.warning(f"Tool '{tool_name}' not found for unregistering.")

    def get_all_tools(self) -> List[BaseTool]:
        return list(self._tools.values())

    def get_openai_tools(self) -> List[Dict]:
        """
        èŽ·å–æ‰€æœ‰å·²æ³¨å†Œå·¥å…·å¹¶è½¬æ¢ä¸ºOpenAI Function Callingæ ¼å¼ã€‚
        """
        return [
            {"type": "function", "function": convert_to_openai_function(t)}
            for t in self.get_all_tools()
        ]


MOCK_FINANCIAL_DATA = {
    "tencent": "609 billion Chinese yuan (Fiscal Year 2025)",
    "alibaba": "868.7 billion Chinese yuan (Fiscal Year 2023)",
    "bytedance": "120 billion US dollars (Fiscal Year 2023)"
}


def fetch_with_jina_reader(target_url: str, api_key: str) -> str | None:
    """
    ä½¿ç”¨ Jina Reader API èŽ·å–å¹¶è¿”å›žç»™å®šURLçš„å†…å®¹ã€‚

    è¿™ä¸ªå‡½æ•°æ¨¡æ‹Ÿäº†ä»¥ä¸‹ curl å‘½ä»¤ï¼š
    curl "https://r.jina.ai/{target_url}" -H "Authorization: Bearer {api_key}"

    å‚æ•°:
    target_url (str): ä½ æƒ³è¦æŠ“å–å†…å®¹çš„åŽŸå§‹ URLã€‚
    api_key (str): ä½ çš„ Jina API å¯†é’¥ã€‚

    è¿”å›ž:
    str: æˆåŠŸæ—¶è¿”å›žèŽ·å–åˆ°çš„é¡µé¢å†…å®¹ï¼ˆé€šå¸¸æ˜¯ Markdown æ ¼å¼çš„æ–‡æœ¬ï¼‰ã€‚
    None: å¦‚æžœè¯·æ±‚å¤±è´¥ï¼Œåˆ™è¿”å›ž Noneã€‚
    """
    # 1. æ‹¼æŽ¥ Jina Reader API çš„ URL
    base_url = "https://r.jina.ai/"
    full_url = f"{base_url}{target_url}"

    # 2. å‡†å¤‡è¯·æ±‚å¤´ (Headers)
    headers = {
        "Authorization": f"Bearer {api_key}"
    }

    print(f"æ­£åœ¨è¯·æ±‚: {full_url}")

    try:
        # 3. å‘é€ GET è¯·æ±‚
        response = requests.get(
            full_url, headers=headers, timeout=60)  # è®¾ç½®60ç§’è¶…æ—¶

        # 4. æ£€æŸ¥å“åº”çŠ¶æ€ç ï¼Œå¦‚æžœä¸æ˜¯ 200 (OK)ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
        response.raise_for_status()

        # 5. å¦‚æžœè¯·æ±‚æˆåŠŸï¼Œè¿”å›žå“åº”çš„æ–‡æœ¬å†…å®¹
        return response.text

    except requests.exceptions.HTTPError as e:
        # å¤„ç† HTTP é”™è¯¯ (ä¾‹å¦‚ 401 Unauthorized, 404 Not Found, 500 Server Error)
        print(f"å‘ç”Ÿ HTTP é”™è¯¯: {e}")
        print(f"çŠ¶æ€ç : {e.response.status_code}")
        print(f"å“åº”å†…å®¹: {e.response.text}")
        return None
    except requests.exceptions.RequestException as e:
        # å¤„ç†å…¶ä»–ç½‘ç»œç›¸å…³çš„é”™è¯¯ (ä¾‹å¦‚ DNS æŸ¥è¯¢å¤±è´¥ï¼Œè¿žæŽ¥è¶…æ—¶)
        print(f"å‘ç”Ÿè¯·æ±‚é”™è¯¯: {e}")
        return None


@tool
def get_url_content(url: str):
    """
    èŽ·å–å¹¶è¿”å›žç»™å®šURLçš„ä¸»è¦æ–‡æœ¬å†…å®¹ã€‚

    å½“ç”¨æˆ·æä¾›ä¸€ä¸ªé“¾æŽ¥å¹¶è¦æ±‚æ€»ç»“ã€æå–ä¿¡æ¯æˆ–åŸºäºŽé“¾æŽ¥å†…å®¹å›žç­”é—®é¢˜æ—¶ï¼Œæ­¤å·¥å…·éžå¸¸æœ‰ç”¨ã€‚å®ƒä½¿ç”¨Jina Reader APIæ¥æå–ç½‘é¡µçš„æ ¸å¿ƒå†…å®¹ã€‚

    Args:
        url: éœ€è¦è¯»å–çš„ç½‘é¡µçš„å®Œæ•´URL (ä¾‹å¦‚: 'https://www.example.com/article')ã€‚
    """
    key = os.getenv("JINA_API_KEY")
    if not key:
        return "Error: JINA_API_KEY environment variable is not set."
    content = fetch_with_jina_reader(url, key)
    if content is None:
        return f"Error: Failed to fetch content from {url}."
    return content


@tool
def get_company_revenue(company_name: str) -> str:
    """Gets the latest reported revenue for a given company."""
    name_lower = company_name.lower()
    return MOCK_FINANCIAL_DATA.get(name_lower, f"Sorry, I don't have financial data for {company_name}.")


@tool
def calculator(expression: str) -> str:
    """
    A calculator that evaluates a mathematical expression.
    Supports addition (+), subtraction (-), multiplication (*), and division (/).
    Example: '100 * (5 + 3)'
    """
    # å®‰å…¨è­¦å‘Š: åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­ä½¿ç”¨ eval() æ˜¯å±é™©çš„ï¼Œå› ä¸ºå®ƒå¯èƒ½æ‰§è¡Œä»»æ„ä»£ç ã€‚
    # ä»…ç”¨äºŽæ¼”ç¤ºç›®çš„ã€‚åœ¨å®žé™…åº”ç”¨ä¸­ï¼Œåº”ä½¿ç”¨æ›´å®‰å…¨çš„è§£æžå™¨ï¼Œå¦‚ numexpr æˆ– ast.literal_evalã€‚
    try:
        allowed_chars = "0123456789+-*/(). "
        if not all(c in allowed_chars for c in expression):
            return "Error: Invalid characters in expression."
        result = eval(expression, {"__builtins__": {}}, {})
        return f"Result: {result}"
    except Exception as e:
        return f"Error evaluating expression: {e}"


# ==============================================================================
# LangGraph æœåŠ¡å±‚
# ==============================================================================

class BaseAgent(ABC):
    @abstractmethod
    def invoke(self, query: str) -> Dict[str, Any]:
        """ä»£ç†çš„ç»Ÿä¸€å…¥å£ç‚¹ã€‚"""
        pass


class SearchAgent(BaseAgent):
    """ä¸€ä¸ªä¸“é—¨è´Ÿè´£å¤„ç†æœç´¢ä»»åŠ¡çš„ç‹¬ç«‹ä»£ç†ã€‚"""

    class SearchAgentState(TypedDict):
        original_query: str
        refined_query: str
        search_results: List[BaseMessage]
        final_answer: str

    def __init__(self, client: OpenAIClient, search_tool: BaseTool):
        self.client = client
        self.tool_node = ToolNode([search_tool])
        workflow = StateGraph(self.SearchAgentState)
        workflow.add_node("refine_query", self._refine_query_node)
        workflow.add_node("execute_search", self._execute_search_node)
        workflow.add_node("synthesize_results", self._synthesize_results_node)

        workflow.set_entry_point("refine_query")
        workflow.add_edge("refine_query", "execute_search")
        workflow.add_edge("execute_search", "synthesize_results")
        workflow.add_edge("synthesize_results", END)

        self.graph = workflow.compile()

    def _execute_search_node(self, state: SearchAgentState) -> Dict[str, Any]:
        refined_query = state['refined_query']
        tool_call_message = AIMessage(
            content="",
            tool_calls=[{
                "name": "search_engine",
                "args": {"query": refined_query},
                "id": f"call_{uuid.uuid4()}"
            }]
        )
        search_results = state.get('search_results', [])
        results = self.tool_node.invoke([tool_call_message])

        search_results.extend(results)
        return {"search_results": search_results}

    def _refine_query_node(self, state: SearchAgentState) -> Dict[str, Any]:
        prompt = textwrap.dedent(f"""
            You are a search query refinement expert. Your task is to take a user's question and rephrase it into a concise, keyword-focused query suitable for a search engine.
            User Question: "{state['original_query']}"
            Refined Query:
        """)
        messages = [{"role": "system", "content": prompt}]
        response = self.client.get_chat_completion(
            messages, ANALYSIS_MODEL, 0.0)
        refined_query = response.get('content', state['original_query'])

        return {"refined_query": refined_query}

    def _synthesize_results_node(self, state: SearchAgentState) -> Dict[str, Any]:
        search_results_str = "\n".join(
            [res.content for res in state['search_results']])
        prompt = textwrap.dedent(f"""
            You are a synthesis expert. Based on the provided search results, answer the user's original question.
            Original Question: "{state['original_query']}"
            Search Results:
            ---
            {search_results_str}
            ---
            Synthesized Answer:
        """)
        messages = [{"role": "system", "content": prompt}]
        response = self.client.get_chat_completion(
            messages, RESPONSE_MODEL, 0.2)
        final_answer = response.get(
            'content', "Could not synthesize an answer.")
        return {"final_answer": final_answer}

    def invoke(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡Œæœç´¢ä»£ç†å·¥ä½œæµã€‚"""
        return self.graph.invoke({"original_query": query})


MOCK_SEARCH_RESULTS = {
    "langgraph": "LangGraph is a library for building stateful, multi-actor applications with LLMs. It extends the LangChain expression language with the ability to coordinate multiple chains (or actors) across multiple steps of computation in a cyclical manner.",
    "rag": "Retrieval-Augmented Generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLMâ€™s internal representation of information.",
    "agent memory": "Agent memory is the capability of an AI agent to retain and recall information from past interactions. This allows the agent to maintain context, learn from experience, and provide more personalized and coherent responses over time."
}


@tool
def search_engine(query: str) -> str:
    """
    Simulates a search engine to find information on the web about a given topic.
    Useful for finding definitions or explanations of concepts like 'LangGraph', 'RAG', or 'Agent Memory'.
    """
    query_lower = query.lower()
    for keyword, result in MOCK_SEARCH_RESULTS.items():
        if keyword in query_lower:
            return f"Search results for '{query}':\n{result}"
    return f"No specific information found for your query '{query}'. Try searching for 'LangGraph', 'RAG', or 'Agent Memory'."


class KnowledgeBase:
    """
    ä¸€ä¸ªå¯æ‰©å±•çš„çŸ¥è¯†åº“ç±»ï¼Œç”¨äºŽç®¡ç†å’Œæ£€ç´¢ä¿¡æ¯ã€‚
    è¿™ä¸ªåŸºç¡€å®žçŽ°ä½¿ç”¨ä¸€ä¸ªç¡¬ç¼–ç çš„æ–‡æœ¬å—ä½œä¸ºçŸ¥è¯†æºã€‚
    å¯ä»¥è¢«ç»§æ‰¿ä»¥å®žçŽ°æ›´å¤æ‚çš„æ£€ç´¢é€»è¾‘ï¼Œä¾‹å¦‚ä»Žæ•°æ®åº“æˆ–å‘é‡å­˜å‚¨ä¸­æ£€ç´¢ã€‚
    """

    def __init__(self):
        # å°†mockæ•°æ®ç§»åˆ°è¿™é‡Œ
        self._knowledge_text = "\n".join([
            "Fact: Tencent is a leading internet and technology company based in China.",
            "Fact: Alibaba Group is a Chinese multinational technology company specializing in e-commerce, retail, Internet, and technology.",
            "Fact: Alibaba Cloud is a subsidiary of Alibaba Group and is the largest cloud computing company in China.",
            "Fact: You should replace the knowledge base with your own relevant documents for best results.",
        ])
        logging.info("çŸ¥è¯†åº“å·²åˆå§‹åŒ–ï¼ˆä½¿ç”¨å†…å­˜ä¸­çš„mockæ•°æ®ï¼‰ã€‚")

    def get_content(self) -> str:
        """
        è¿”å›žçŸ¥è¯†åº“çš„å…¨éƒ¨å†…å®¹ã€‚
        åœ¨æ›´é«˜çº§çš„å®žçŽ°ä¸­ï¼Œè¿™é‡Œå¯ä»¥æ˜¯åŸºäºŽæŸ¥è¯¢çš„æ£€ç´¢ã€‚
        """
        return self._knowledge_text


class LangGraphService:

    def __init__(self, client: OpenAIClient, prompter: PromptFactory, history_manager: HistoryManager, tool_manager: ToolManager, knowledge_base: KnowledgeBase, search_agent: BaseAgent):
        self.client = client
        self.prompter = prompter
        self.history_manager = history_manager
        self.tool_manager = tool_manager
        self.knowledge_base = knowledge_base
        self.openai_tools = self.tool_manager.get_openai_tools()
        self.tool_node = ToolNode(self.tool_manager.get_all_tools())

        workflow = StateGraph(GraphState)
        workflow.add_node("manage_history", self.manage_history_node)
        workflow.add_node("analyze_situation", self.analyze_situation_node)

        workflow.add_node("update_profile", self.update_profile_node)
        workflow.add_node("determine_strategy", self.determine_strategy_node)
        workflow.add_node("retrieve_knowledge", self.retrieve_knowledge_node)
        workflow.add_node("generate_response", self.generate_response_node)
        workflow.add_node("execute_tools", self.tool_executor)
        workflow.add_node("delegate_to_search_agent",
                          self.delegate_to_search_agent_node)
        workflow.add_node("describe_image", self.describe_image_node)

        workflow.set_entry_point("manage_history")

        workflow.add_conditional_edges(
            "manage_history",
            lambda state: "describe_image" if state.get(
                "image_b64_urls") else "analyze_situation",
            {
                "describe_image": "describe_image",
                "analyze_situation": "analyze_situation"
            }
        )

        workflow.add_edge("describe_image", "analyze_situation")

        workflow.add_conditional_edges(
            "analyze_situation",
            self.route_after_analysis,
            {
                "update_profile": "update_profile",
                "continue": "determine_strategy"
            }
        )

        workflow.add_edge("update_profile",
                          "determine_strategy")

        workflow.add_conditional_edges(
            "determine_strategy",
            self.route_after_strategy,
            {
                "rag": "retrieve_knowledge",
                "tool": "execute_tools",
                "direct": "generate_response",
                "search_agent": "delegate_to_search_agent",
            }
        )

        workflow.add_edge("retrieve_knowledge", "generate_response")
        workflow.add_edge("execute_tools", "generate_response")
        workflow.add_edge("delegate_to_search_agent", "generate_response")
        workflow.add_edge("generate_response", END)

        self.graph = workflow.compile()

        self.search_agent = search_agent

    def describe_image_node(self, state: GraphState) -> Dict[str, Any]:
        """å¦‚æžœæä¾›äº†å›¾ç‰‡ï¼Œåˆ™è°ƒç”¨å¤šæ¨¡æ€æ¨¡åž‹ç”Ÿæˆç»Ÿä¸€æè¿°å¹¶æ·»åŠ åˆ°åŽ†å²è®°å½•ä¸­ã€‚"""
        image_b64_list = state.get("image_b64_urls")
        if not image_b64_list:
            logging.info("æœªæä¾›å›¾ç‰‡ï¼Œè·³è¿‡å›¾ç‰‡æè¿°èŠ‚ç‚¹ã€‚")
            return {}

        logging.info(f"æ£€æµ‹åˆ° {len(image_b64_list)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ç”Ÿæˆç»Ÿä¸€æè¿°...")
        prompt = "è¯·è¯¦ç»†æè¿°ä»¥ä¸‹æ‰€æœ‰å›¾ç‰‡çš„å†…å®¹ã€‚å¦‚æžœå›¾ç‰‡ä¹‹é—´æœ‰å…³è”ï¼Œè¯·è¯´æ˜Žå®ƒä»¬çš„å…³ç³»ã€‚è¯·ç»¼åˆæ‰€æœ‰ä¿¡æ¯ï¼Œç»™å‡ºä¸€ä¸ªè¿žè´¯çš„æè¿°ã€‚"
        description = self.client.describe_image(
            image_b64_list, prompt, VISION_MODEL)

        logging.info(f"å›¾ç‰‡æè¿°ç”Ÿæˆ: {description}")

        # å°†å›¾ç‰‡æè¿°ä½œä¸ºä¸€æ¡å·¥å…·æ¶ˆæ¯æ³¨å…¥åˆ°å¯¹è¯åŽ†å²ä¸­
        image_context_message = ToolMessage(
            name="ä¸Šä¼ çš„å›¾ç‰‡æè¿°",
            tool_call_id=str(uuid.uuid4()),
            content=f"ç”¨æˆ·æä¾›äº† {len(image_b64_list)} å¼ å›¾ç‰‡ï¼Œå…¶ç»¼åˆå†…å®¹æè¿°å¦‚ä¸‹ï¼š\n---\n{description}\n---"
        )
        return {"conversation_history": [image_context_message], "image_b64_urls": []}

    def delegate_to_search_agent_node(self, state: GraphState) -> Dict[str, Any]:
        logging.info("å§”æ‰˜ä»»åŠ¡ç»™æœç´¢ä»£ç†...")
        original_query = state['plan'].search_query

        agent_result = self.search_agent.invoke(original_query)

        synthesized_answer = agent_result.get(
            'final_answer', "æœç´¢ä»£ç†æœªèƒ½è¿”å›žç»“æžœã€‚")
        logging.info(f"æœç´¢ä»£ç†è¿”å›žç»“æžœ: {synthesized_answer}")

        return {"rag_context": synthesized_answer}

    def tool_executor(self, state: GraphState) -> GraphState:
        tool_calls_message = state['conversation_history'][-1]
        result = self.tool_node.invoke([tool_calls_message])
        state['conversation_history'].extend(result)
        return state

    def _clean_and_parse_json(self, raw_response: str) -> Optional[Dict[str, Any]]:
        match = re.search(r"```json\s*([\s\S]*?)\s*```", raw_response)
        if match:
            json_str = match.group(1)
        else:
            start = raw_response.find('{')
            end = raw_response.rfind('}')
            if start != -1 and end != -1 and end > start:
                json_str = raw_response[start:end+1]
            else:
                json_str = raw_response
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            logging.error(f"JSONè§£æžå¤±è´¥: {e}\nåŽŸå§‹å­—ç¬¦ä¸²: '{json_str}'")
            return None

    def _call_llm_for_json_content(self, prompt: str, model: str, temperature: float) -> Optional[Dict[str, Any]]:
        messages = [{"role": "system", "content": prompt}]
        response_message = self.client.get_chat_completion(
            messages, model, temperature, is_json=True)
        content = response_message.get("content", "")
        if not content:
            return {"error": "LLM returned an empty response."}
        return self._clean_and_parse_json(content)

    def manage_history_node(self, state: GraphState) -> Dict[str, Any]:
        logging.info("ç®¡ç†å¯¹è¯åŽ†å²èŠ‚ç‚¹è¢«è§¦å‘ã€‚")
        history = state['conversation_history']
        compressed_history = self.history_manager.compress_history_if_needed(
            history)
        return {"conversation_history": compressed_history}

    def _deep_merge_dicts(self, base: dict, update: dict) -> dict:
        merged = deepcopy(base)
        for key, value in update.items():
            if key in merged and isinstance(merged.get(key), dict) and isinstance(value, dict):
                # é€’å½’åˆå¹¶åµŒå¥—å­—å…¸
                merged[key] = self._deep_merge_dicts(merged[key], value)

            elif key in merged and isinstance(merged.get(key), list) and isinstance(value, list):
                # --- å¼€å§‹ï¼šåˆ—è¡¨åˆå¹¶çš„æ™ºèƒ½é€»è¾‘ ---
                base_list = merged[key]
                update_list = value

                # æ£€æŸ¥åˆ—è¡¨æ˜¯å¦åŒ…å«å­—å…¸ï¼Œä»¥å†³å®šåˆå¹¶ç­–ç•¥
                is_list_of_dicts = (base_list and isinstance(base_list[0], dict)) or \
                                   (update_list and isinstance(
                                       update_list[0], dict))

                if is_list_of_dicts:
                    # ç­–ç•¥1: åˆå¹¶å­—å…¸åˆ—è¡¨ (Upsert logic)
                    # å°è¯•ä¸ºå¯¹è±¡æ‰¾åˆ°ä¸€ä¸ªå”¯ä¸€é”®ï¼ˆå¦‚IDæˆ–åç§°ï¼‰
                    potential_unique_keys = [
                        'skillName', 'item', 'title', 'name', 'id']
                    unique_key = None

                    sample_item = next(
                        (item for item in base_list + update_list if item), None)
                    if sample_item:
                        for pk in potential_unique_keys:
                            if pk in sample_item:
                                unique_key = pk
                                break

                    if unique_key:
                        # ä½¿ç”¨å­—å…¸è¿›è¡Œé«˜æ•ˆçš„æ›´æ–°æˆ–æ’å…¥æ“ä½œ
                        merged_map = {
                            item[unique_key]: item for item in base_list if unique_key in item}
                        for item in update_list:
                            if unique_key in item:
                                merged_map[item[unique_key]] = item  # è¦†ç›–æˆ–æ·»åŠ 
                        merged[key] = list(merged_map.values())
                    else:
                        # å¦‚æžœæ‰¾ä¸åˆ°å”¯ä¸€é”®ï¼Œåˆ™ç›´æŽ¥è¿½åŠ æ–°é¡¹ç›®ï¼ˆé¿å…åŽ»é‡ï¼‰
                        merged[key].extend(update_list)
                else:
                    # ç­–ç•¥2: åˆå¹¶ç®€å•ç±»åž‹ï¼ˆå¦‚å­—ç¬¦ä¸²ï¼‰çš„åˆ—è¡¨å¹¶åŽ»é‡
                    try:
                        combined_list = base_list + update_list
                        # ä½¿ç”¨ dict.fromkeys å®‰å…¨åŽ»é‡
                        merged[key] = list(dict.fromkeys(combined_list))
                    except TypeError:
                        # å¦‚æžœé‡åˆ°å…¶ä»–ä¸å¯å“ˆå¸Œç±»åž‹ï¼Œåˆ™ç›´æŽ¥è¿½åŠ 
                        merged[key].extend(update_list)
                # --- ç»“æŸï¼šåˆ—è¡¨åˆå¹¶çš„æ™ºèƒ½é€»è¾‘ ---
            else:
                # å¯¹äºŽå…¶ä»–æ‰€æœ‰æƒ…å†µï¼ˆæ–°é”®æˆ–ç±»åž‹ä¸åŒ¹é…ï¼‰ï¼Œç›´æŽ¥è¦†ç›–
                merged[key] = value
        return merged

    def analyze_situation_node(self, state: GraphState) -> Dict[str, Any]:
        logging.info("æƒ…æ™¯åˆ†æžèŠ‚ç‚¹è¢«è§¦å‘ã€‚")
        prompt = self.prompter.create_situational_analysis_prompt(
            state['user_input'], state['conversation_history'], state['user_profile'])
        data = self._call_llm_for_json_content(prompt, ANALYSIS_MODEL, 0.1)

        if not data:
            data = {}

        try:
            analysis_output = SituationalAnalysisOutput.model_validate(data)
            snapshot = SituationalSnapshot(
                who_profile=state['user_profile'],
                what_topic=analysis_output.what_topic_analysis,
                why_intent=analysis_output.why_intent_analysis,
                how_demand=analysis_output.how_demand
            )
            return {"snapshot": snapshot}
        except ValidationError as e:
            logging.error(f"æƒ…æ™¯åˆ†æžèŠ‚ç‚¹ Pydantic æ ¡éªŒå¤±è´¥: {e}\næŽ¥æ”¶åˆ°çš„æ•°æ®: {data}")
            snapshot = SituationalSnapshot(who_profile=state['user_profile'], what_topic=TopicAnalysis(
            ), why_intent=IntentAnalysis(), how_demand=CoreDemand())
            return {"snapshot": snapshot}

    def update_profile_node(self, state: GraphState) -> Dict[str, Any]:
        logging.info("å‘¨æœŸæ€§ç”¨æˆ·ç”»åƒæ›´æ–°èŠ‚ç‚¹è¢«è§¦å‘ã€‚")
        prompt = self.prompter.create_profile_update_prompt(
            state['conversation_history'], state['user_profile'])

        profile_update_data = self._call_llm_for_json_content(
            prompt, PROFILE_MODEL, 0.2)

        if not profile_update_data or "error" in profile_update_data:
            logging.warning("ç”»åƒæ›´æ–° LLM è°ƒç”¨å¤±è´¥æˆ–è¿”å›žç©ºï¼Œè·³è¿‡æ›´æ–°ã€‚")
            return {}  # ä¸åšä»»ä½•ä¿®æ”¹

        try:
            # ä¸å†éœ€è¦ sanitizeï¼Œç›´æŽ¥åˆå¹¶
            current_profile_dict = state['user_profile'].model_dump()
            updated_profile_dict = self._deep_merge_dicts(
                current_profile_dict, profile_update_data)  # ç›´æŽ¥ä½¿ç”¨LLMçš„è¾“å‡º
            updated_profile = UserProfile.model_validate(updated_profile_dict)

            logging.info("ç”¨æˆ·ç”»åƒæ›´æ–°æˆåŠŸã€‚")
            # æ›´æ–°å¿«ç…§ä¸­çš„ç”»åƒéƒ¨åˆ†ï¼Œä»¥ä¾¿åŽç»­èŠ‚ç‚¹èƒ½ç”¨åˆ°æœ€æ–°çš„ç”»åƒ
            updated_snapshot = state['snapshot'].model_copy(
                update={'who_profile': updated_profile})

            return {"user_profile": updated_profile, "snapshot": updated_snapshot}
        except ValidationError as e:
            logging.error(
                f"ç”»åƒæ›´æ–° Pydantic æ ¡éªŒå¤±è´¥: {e}\næŽ¥æ”¶åˆ°çš„æ•°æ®: {profile_update_data}")
            return {}  # æ ¡éªŒå¤±è´¥ï¼Œä¿æŒåŽŸç”»åƒ

    def format_history_for_prompt(self, history: List[BaseMessage]) -> str:
        def get_text_from_content(content):
            if isinstance(content, str):
                return content
            elif isinstance(content, list):
                return " ".join(
                    part["content"] for part in content if part["type"] == "text"
                )
            return ""
        history_str = "\n".join(
            f"{MESSAGE_TYPE[msg.type]}: {get_text_from_content(msg.content)}" for msg in history)
        return history_str

    def determine_strategy_node(self, state: GraphState) -> Dict[str, Any]:

        logging.info("è¿›å…¥ determine_strategy_nodeï¼Œå¼€å§‹åˆ¶å®šè¡ŒåŠ¨ç­–ç•¥ã€‚")

        history = state['conversation_history'][-6:]  # ä»…å–æœ€è¿‘6æ¡æ¶ˆæ¯
        history_str = self.history_manager.format_history_for_model(history)

        prompt = self.prompter.create_strategy_prompt(
            state['snapshot'], history_str, self.tool_manager.get_openai_tools())
        data = self._call_llm_for_json_content(prompt, STRATEGY_MODEL, 0.3)

        if not data:
            plan = ActionPlan(strategy=ActionStrategy.DIRECT_ANSWER, reasoning="Fallback strategy due to error.",
                              proposed_response_outline="Address user directly.")
            return {"plan": plan}

        try:
            plan = ActionPlan.model_validate(data)

            if plan.strategy == ActionStrategy.TOOL_USE and plan.tool_calls:

                tool_calls_for_message = [
                    {
                        "name": tc.name,
                        "args": tc.parameters,
                        "id": f"call_{uuid.uuid4()}"
                    } for tc in plan.tool_calls
                ]

                ai_message_with_tool_calls = AIMessage(
                    content="",
                    tool_calls=tool_calls_for_message
                )
                return {"plan": plan, "conversation_history": [ai_message_with_tool_calls]}

            return {"plan": plan}
        except ValidationError as e:
            logging.error(f"ç­–ç•¥èŠ‚ç‚¹ Pydantic æ ¡éªŒå¤±è´¥: {e}")
            plan = ActionPlan(strategy=ActionStrategy.DIRECT_ANSWER, reasoning="Fallback strategy due to validation error.",
                              proposed_response_outline="Address user directly.")
            return {"plan": plan}

    def retrieve_knowledge_node(self, state: GraphState) -> Dict[str, Any]:
        search_query = state['plan'].search_query
        knowledge_content = self.knowledge_base.get_content()
        prompt = self.prompter.create_rag_retrieval_prompt(
            knowledge_content, search_query)
        messages = [{"role": "system", "content": prompt}]
        response_message = self.client.get_chat_completion(
            messages, RETRIEVAL_MODEL, 0.0)
        retrieved_text = response_message.get(
            "content", "Failed to retrieve from knowledge base.")
        return {"rag_context": retrieved_text}

    def generate_response_node(self, state: GraphState) -> Dict[str, Any]:
        logging.info("è¿›å…¥ generate_response_nodeï¼Œå¼€å§‹åˆæˆæœ€ç»ˆå›žå¤ã€‚")
        tool_outputs = []
        for msg in reversed(state['conversation_history']):
            if isinstance(msg, ToolMessage) or isinstance(msg, FunctionMessage):
                tool_outputs.append(
                    {"tool_name": msg.name, "output": msg.content})
            elif isinstance(msg, ChatMessage) and msg.tool_calls:
                break

        prompt = self.prompter.create_responder_prompt(
            state['user_input'],
            state['snapshot'],
            state['plan'],
            state['active_persona'],
            state.get('rag_context'),
            tool_outputs if tool_outputs else None
        )

        messages = [{"role": "system", "content": prompt}]

        # ä½¿ç”¨éžJSONæ¨¡å¼è°ƒç”¨ï¼Œå› ä¸ºæˆ‘ä»¬æœŸæœ›çš„æ˜¯è‡ªç„¶è¯­è¨€æ–‡æœ¬
        response_message = self.client.get_chat_completion(
            messages, RESPONSE_MODEL, 0.7, is_json=False)

        final_response = response_message.get("content", "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›žå¤ã€‚")

        # å°†æœ€ç»ˆå›žå¤åŒ…è£…æˆ AIMessage æ·»åŠ åˆ°åŽ†å²è®°å½•
        final_ai_message = AIMessage(content=final_response)

        return {
            "final_response": final_response,
            "conversation_history": [final_ai_message]
        }

    def route_after_analysis(self, state: GraphState) -> str:
        turn_count = state.get('turn_count', 0)
        if turn_count > 0 and turn_count % PROFILE_UPDATE_INTERVAL == 0:
            logging.info(f"ç¬¬ {turn_count} è½®å¯¹è¯ï¼šè·¯ç”±åˆ° 'update_profile'ã€‚")
            return "update_profile"
        else:
            logging.info(f"ç¬¬ {turn_count} è½®å¯¹è¯ï¼šè·¯ç”±åˆ° 'determine_strategy'ã€‚")
            return "continue"

    def route_after_strategy(self, state: GraphState) -> str:

        strategy = state['plan'].strategy
        logging.info(f"è·¯ç”±å†³ç­–ï¼šæ£€æµ‹åˆ°ç­–ç•¥ä¸º '{strategy.value}'ã€‚")

        if strategy == ActionStrategy.RAG_QUERY:
            logging.info("è·¯ç”±åˆ° 'retrieve_knowledge' (rag)ã€‚")
            return "rag"
        elif strategy == ActionStrategy.TOOL_USE and state['plan'].tool_calls:
            logging.info("è·¯ç”±åˆ° 'execute_tools' (tool)ã€‚")
            return "tool"
        elif strategy == ActionStrategy.SEARCH_AGENT_DELEGATION:
            return "search_agent"
        else:  # DIRECT_ANSWER, CLARIFY_ASK, PROACTIVE_GUIDE
            logging.info("è·¯ç”±åˆ° 'generate_response' (direct)ã€‚")
            return "direct"

    def stream(self, **kwargs):
        """Wrapper for the graph stream method."""
        return self.graph.stream(kwargs)
# ==============================================================================
# UI å±‚ (Gradio Application)
# ==============================================================================


class ChatApplication:
    def __init__(self, service: LangGraphService, session_manager: SessionManager, persona_manager: PersonaManager):
        self.service = service
        self.session_manager = session_manager
        self.persona_manager = persona_manager

    def _handle_file_upload(self, file):
        """å½“æ–‡ä»¶ä¸Šä¼ æ—¶ï¼Œæ›´æ–°æŒ‰é’®çŠ¶æ€ä»¥æä¾›åé¦ˆã€‚"""
        if file:
            return gr.update(interactive=False)
        return gr.update()

    def _convert_langchain_to_gradio(self, messages: List[BaseMessage]) -> List[Dict[str, any]]:
        history = []

        def concat_human_message(msg: HumanMessage):
            content = msg.content
            if isinstance(msg.content, str):
                return msg.content
            elif isinstance(msg.content, list):
                text_for_display = ""
                for part in content:
                    if part["type"] == "text":
                        text_for_display = part["content"]
                    elif part["type"] == "image":
                        text_for_display += f'<img src="{part["content"]}" style="max-width:300px;"/>'
                return text_for_display
            return ""

        for msg in messages:
            if not (isinstance(msg, (HumanMessage, AIMessage)) and msg.content):
                continue

            if isinstance(msg, HumanMessage):
                history.append(
                    {"role": "user", "content": concat_human_message(msg)})

            elif isinstance(msg, AIMessage):
                history.append({"role": "assistant", "content": msg.content})

        return history

    def add_new_session(self):
        session_id, _ = self.session_manager.create_session()
        session_list = self.session_manager.get_session_list()
        return (
            gr.Radio(choices=session_list, value=session_id),
            session_id,
            [],
            gr.update(value={}),
            gr.update(value="...")
        )

    def update_session_persona(self, session_id: str, persona_name: str):
        if not session_id:
            return
        self.session_manager.update_persona_for_session(
            session_id, persona_name)

    def switch_session(self, selected_session_id: str):
        if not selected_session_id:
            default_persona = self.persona_manager.get_default_persona().name
            return None, [], gr.update(value={}), gr.update(value="..."), gr.update(value=default_persona)

        session_data = self.session_manager.get_session_data(
            selected_session_id)

        history = []
        persona_name = self.persona_manager.get_default_persona().name

        if session_data:
            history = self._convert_langchain_to_gradio(
                session_data.get("conversation_history", []))
            persona_name = session_data.get(
                "active_persona_name", persona_name)

        return (
            selected_session_id,
            history,
            gr.update(value={}),
            gr.update(value="..."),
            gr.update(value=persona_name)
        )

    def delete_selected_session(self, session_to_delete_id: str):
        if not session_to_delete_id:
            return gr.update(), None, [], gr.update(value={}), gr.update(value="..."), gr.update()

        self.session_manager.delete_session(session_to_delete_id)
        session_list = self.session_manager.get_session_list()
        new_selected_id = session_list[0][1] if session_list else None

        history = []
        persona_name = self.persona_manager.get_default_persona().name

        if new_selected_id:
            session_data = self.session_manager.get_session_data(
                new_selected_id)
            if session_data:
                history = self._convert_langchain_to_gradio(
                    session_data.get("conversation_history", []))
                persona_name = session_data.get(
                    "active_persona_name", persona_name)

        return (
            gr.Radio(choices=session_list, value=new_selected_id),
            new_selected_id,
            history,
            gr.update(value={}),
            gr.update(value="..."),
            gr.update(value=persona_name)
        )

    def chat_flow(self, user_input: str, uploaded_file: Optional[str], current_session_id: str) -> Iterator:
        if (not user_input or not user_input.strip()) and not uploaded_file:
            yield [], gr.update(), gr.update()
            return

        if not current_session_id:
            raise gr.Error(
                "No active session. Please create one using the '+ New Session' button.")

        session_data = self.session_manager.get_session_data(
            current_session_id)
        if not session_data:
            raise gr.Error(
                f"Critical error: Could not find data for session ID {current_session_id}")

        llm_history = session_data.get("conversation_history", [])
        display_history = self._convert_langchain_to_gradio(llm_history)

        image_b64_urls = []
        ui_image_parts = []

        url_matches = re.findall(
            r'https?://\S+\.(?:jpg|jpeg|png|gif|webp|bmp)', user_input, re.IGNORECASE)
        if url_matches:
            for url in url_matches:
                image_data = self._get_image_from_url(url)
                if image_data:
                    image_bytes, mime_type = image_data
                    b64_string = base64.b64encode(image_bytes).decode('utf-8')
                    data_url = f"data:{mime_type};base64,{b64_string}"
                    image_b64_urls.append(data_url)
                    ui_image_parts.append({"type": "image", "image": data_url})

        if uploaded_file:
            try:
                with open(uploaded_file, "rb") as f:
                    image_bytes = f.read()
                mime_type, _ = mimetypes.guess_type(uploaded_file) or (
                    "application/octet-stream", None)
                b64_string = base64.b64encode(image_bytes).decode('utf-8')
                data_url = f"data:{mime_type};base64,{b64_string}"
                image_b64_urls.append(data_url)
                ui_image_parts.append({"type": "image", "image": data_url})
            except Exception as e:
                logging.error(f"Error processing uploaded file: {e}")

        text_for_display = ""
        content_for_msg = []
        for img in image_b64_urls:
            text_for_display += f'<img src="{img}" style="max-width:300px;"/>'
            content_for_msg.append({"type": "image", "content": img})
        content_for_llm = user_input.strip()
        if not content_for_llm and image_b64_urls:
            content_for_llm = "è¯·è¯¦ç»†æè¿°å›¾ç‰‡å†…å®¹ã€‚"
            user_input = content_for_llm
        else:
            text_for_display += f"\n{user_input.strip()}"
            content_for_msg.append(
                {"type": "text", "content": user_input.strip()})

        if len(content_for_msg) == 1 and content_for_msg[0]["type"] == "text":
            content_for_msg = content_for_msg[0]["content"]



        if text_for_display:
            display_history.append(
                {"role": "user", "content": text_for_display})
            llm_history.append(HumanMessage(content=content_for_msg))

        display_history.append(
            {"role": "assistant", "content": "ðŸ§  Thinking..."})
        yield display_history, gr.update(value={}), gr.update(value="Thinking...")

        active_persona_name = session_data.get(
            "active_persona_name", self.persona_manager.get_default_persona().name)
        active_persona = self.persona_manager.get_persona(active_persona_name)

        graph_input = {
            "conversation_history": llm_history,
            "user_profile": session_data["user_profile"],
            "user_input": user_input,
            "turn_count": session_data["turn_count"],
            "active_persona": active_persona,
        }
        
        
        
        if image_b64_urls:
            graph_input["image_b64_urls"] = image_b64_urls

        final_state, snapshot_json, plan_md = {}, {}, "..."
        for event in self.service.stream(**graph_input):
            node_name = list(event.keys())[0]
            node_output = event[node_name]
            if node_output:
                final_state.update(node_output)
            else:
                continue
            assistant_status_message = display_history[-1]["content"]

            if node_name == 'describe_image' and 'conversation_history' in node_output:
                assistant_status_message = "ðŸ–¼ï¸ Analyzing image(s)..."

            if 'snapshot' in node_output:
                assistant_status_message = "ðŸ¤” Analyzing context..."
                snapshot_json = final_state['snapshot'].model_dump(
                    exclude_unset=True)

            if node_name == 'update_profile' and 'user_profile' in node_output:
                assistant_status_message = "ðŸ”„ Consolidating user profile..."
                if 'snapshot' in node_output:
                    snapshot_json = node_output['snapshot'].model_dump_json(
                        indent=2, exclude_unset=True)

            if 'plan' in node_output:
                assistant_status_message = "ðŸŽ¯ Deciding strategy..."
                plan = final_state['plan']
                plan_md = f"**Strategy:** `{plan.strategy.value}`\n\n**Reasoning:**\n{plan.reasoning}"

            if 'rag_context' in node_output:
                plan_md += f"\n\n**Retrieved Context:**\n```\n{node_output['rag_context']}\n```"
                assistant_status_message = "ðŸ“š Retrieving knowledge..."

            if node_name == 'execute_tools':
                tool_output_messages = node_output.get(
                    'conversation_history', [])
                if tool_output_messages:
                    messages_to_display = tool_output_messages[-2:]
                    display_count = len(messages_to_display)
                    plan_md += f"\n\n**Tool Output (Last {display_count}):**\n```json\n{json.dumps([m.model_dump() for m in messages_to_display], indent=2, ensure_ascii=False)}\n```"
                assistant_status_message = "âš™ï¸ Executing tools..."

            display_history[-1]["content"] = assistant_status_message
            yield display_history, gr.update(value=snapshot_json), gr.update(value=plan_md)

        final_response = final_state.get(
            "final_response", "Sorry, an error occurred.")
        display_history[-1]["content"] = final_response

        final_history = llm_history + \
            final_state.get('conversation_history', [])[-1:]
        final_state['conversation_history'] = final_history
        self.session_manager.update_session_data(
            current_session_id, final_state)

        yield display_history, gr.update(value=snapshot_json), gr.update(value=plan_md)

    def _get_image_from_url(self, url: str) -> Optional[Tuple[bytes, str]]:
        """ä»ŽURLä¸‹è½½å›¾ç‰‡å¹¶è¿”å›žå…¶å­—èŠ‚å’ŒMIMEç±»åž‹ã€‚"""
        try:
            # æ·»åŠ å¸¸è§çš„User-Agentï¼Œé˜²æ­¢ä¸€äº›ç½‘ç«™é˜»æ­¢è¯·æ±‚
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
            response = requests.get(
                url, stream=True, timeout=10, headers=headers)
            response.raise_for_status()
            
            content_type = response.headers.get('Content-Type')
            if content_type and content_type.startswith('image'):
                return response.content, content_type
            logging.warning(f"URL {url} çš„ Content-Type ({content_type}) ä¸æ˜¯å›¾ç‰‡ã€‚")
            return None
        except requests.exceptions.RequestException as e:
            logging.error(f"ä»ŽURLä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
            return None

    def launch(self):
        css = """
        #chatbot { min-height: 75vh; }
        """
        sessions = self.session_manager.get_session_list()
        initial_session_id = sessions[0][1] if sessions else None
        with gr.Blocks(theme=gr.themes.Soft(), css=css) as demo:
            current_session_id_state = gr.State(initial_session_id)

            gr.Markdown("# ðŸ¤– æ›´æ‡‚ä½ çš„AI")

            with gr.Row():
                with gr.Column(scale=1, min_width=80) as sidebar_col:

                    with gr.Group(visible=True) as expanded_sidebar:
                        with gr.Row(equal_height=False):
                            gr.Markdown("### ä¼šè¯ç®¡ç†")
                            collapse_btn = gr.Button(
                                "áŠ æŠ˜å ", size="sm", variant="ghost", min_width=60)

                        initial_session_data = self.session_manager.get_session_data(
                            initial_session_id)
                        initial_persona = initial_session_data.get(
                            "active_persona_name") if initial_session_data else self.persona_manager.get_default_persona().name
                        persona_dd = gr.Dropdown(
                            label="AIäººæ ¼",
                            choices=self.persona_manager.list_persona_names(),
                            value=initial_persona,
                            interactive=True
                        )
                        gr.Markdown("---")
                        add_session_btn = gr.Button(
                            "+ æ–°å»ºä¼šè¯", variant="primary", size="sm")
                        initial_session_list = self.session_manager.get_session_list()
                        session_list_radio = gr.Radio(
                            label="å½“å‰ä¼šè¯åˆ—è¡¨",
                            choices=initial_session_list,
                            value=initial_session_id,
                            interactive=True
                        )
                        delete_session_btn = gr.Button("åˆ é™¤é€‰ä¸­ä¼šè¯", size="sm")

                    with gr.Group(visible=False) as collapsed_sidebar:
                        expand_btn = gr.Button("á…", size="sm", variant="ghost")

                with gr.Column(scale=8) as main_col:
                    with gr.Row():
                        with gr.Column(scale=2):
                            chatbot = gr.Chatbot(
                                label="å¯¹è¯çª—å£", elem_id="chatbot", show_copy_button=True, type='messages'
                            )
                            with gr.Row():

                                upload_btn = gr.UploadButton(
                                    "ðŸ–¼ï¸ ä¸Šä¼ å›¾ç‰‡", file_types=["image"], elem_id="upload_button"
                                )
                                user_input_textbox = gr.Textbox(
                                    placeholder="åœ¨è¿™é‡Œè¾“å…¥ä½ çš„æ¶ˆæ¯...", label="ä½ çš„æ¶ˆæ¯", container=False, scale=4
                                )
                                send_btn = gr.Button(
                                    "å‘é€", variant="primary", scale=1)
                        with gr.Column(scale=1, min_width=300):
                            gr.Markdown("### ðŸ§  AI åœ¨æƒ³ä»€ä¹ˆ")
                            with gr.Accordion("æƒ…æ™¯å¿«ç…§ & ç”¨æˆ·ç”»åƒ", open=True):
                                snapshot_view = gr.JSON(label="åˆ†æžç»“æžœ", value={})
                            with gr.Accordion("è¡ŒåŠ¨è§„åˆ’ & çŸ¥è¯†/å·¥å…·/æœç´¢", open=True):
                                plan_view = gr.Markdown(value="ç­‰å¾…è¾“å…¥...")

            def collapse_sidebar():
                return (
                    gr.update(visible=False),
                    gr.update(visible=True),
                    gr.update(scale=0, min_width=60)
                )

            def expand_sidebar():
                return (
                    gr.update(visible=True),
                    gr.update(visible=False),
                    gr.update(scale=1, min_width=250)
                )

            collapse_btn.click(
                fn=collapse_sidebar,
                inputs=None,
                outputs=[expanded_sidebar, collapsed_sidebar, sidebar_col]
            )
            expand_btn.click(
                fn=expand_sidebar,
                inputs=None,
                outputs=[expanded_sidebar, collapsed_sidebar, sidebar_col]
            )

            persona_dd.change(
                fn=self.update_session_persona,
                inputs=[current_session_id_state, persona_dd],
                outputs=None
            )

            def add_new_session_ui_update():
                session_id, _ = self.session_manager.create_session()
                session_list = self.session_manager.get_session_list()
                default_persona = self.persona_manager.get_default_persona().name
                return (
                    gr.Radio(choices=session_list, value=session_id),
                    session_id,
                    [], gr.update(value={}), gr.update(value="..."),
                    gr.Dropdown(value=default_persona)  # Reset dropdown
                )

            ui_outputs = [session_list_radio, current_session_id_state,
                          chatbot, snapshot_view, plan_view, persona_dd]
            add_session_btn.click(
                add_new_session_ui_update, outputs=ui_outputs)
            session_list_radio.change(inputs=[session_list_radio], outputs=[
                                      current_session_id_state, chatbot, snapshot_view, plan_view, persona_dd])
            delete_session_btn.click(self.delete_selected_session, inputs=[
                                     current_session_id_state], outputs=ui_outputs)

            chat_inputs = [user_input_textbox,
                           upload_btn, current_session_id_state]
            chat_outputs = [chatbot, snapshot_view, plan_view]

            upload_btn.upload(
                self._handle_file_upload,
                inputs=[upload_btn],
                outputs=[upload_btn]
            )

            def clear_and_reenable_inputs():
                return "", gr.update(value=None, interactive=True)

            send_btn.click(
                self.chat_flow,
                inputs=chat_inputs,
                outputs=chat_outputs
            ).then(
                clear_and_reenable_inputs,
                outputs=[user_input_textbox, upload_btn]
            )

            user_input_textbox.submit(
                self.chat_flow,
                inputs=chat_inputs,
                outputs=chat_outputs
            ).then(
                clear_and_reenable_inputs,
                outputs=[user_input_textbox, upload_btn]
            )
        demo.launch()


# ==============================================================================
# ä¸»ç¨‹åºå…¥å£
# ==============================================================================
if __name__ == "__main__":
    try:

        openai_client = OpenAIClient(api_key=API_KEY, base_url=BASE_URL)
        prompt_factory = PromptFactory()
        history_manager = HistoryManager(client=openai_client)
        persona_manager = PersonaManager()
        tool_manager = ToolManager()
        tool_manager.register_tool(get_company_revenue)
        tool_manager.register_tool(calculator)
        tool_manager.register_tool(get_url_content)
        knowledge_base = KnowledgeBase()
        search_agent = SearchAgent(
            client=openai_client, search_tool=search_engine)

        langgraph_service = LangGraphService(
            client=openai_client,
            prompter=prompt_factory,
            history_manager=history_manager,
            tool_manager=tool_manager,
            knowledge_base=knowledge_base,
            search_agent=search_agent,
        )

        session_manager = SessionManager()

        session_list = session_manager.get_session_list()
        if not session_list:
            logging.info("è‡ªåŠ¨åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ä¼šè¯ã€‚")
            initial_session_id, _ = session_manager.create_session()

        app = ChatApplication(service=langgraph_service,
                              session_manager=session_manager,
                              persona_manager=persona_manager,)
        app.launch()

    except ValueError as e:
        print(f"é…ç½®é”™è¯¯: {e}")
    except Exception as e:
        logging.critical(f"åº”ç”¨ç¨‹åºå¯åŠ¨å¤±è´¥: {e}", exc_info=True)
        print(
            f"FATAL: Application failed to start. Check logs for details. Error: {e}")
